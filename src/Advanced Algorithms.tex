\documentclass[a4paper, 12pt]{report}

\usepackage[dvipsnames]{xcolor}

%%%%%%%%%%%%%%%%
% Set Variables %
%%%%%%%%%%%%%%%%

\def\useItalian{0}  % 1 = Italian, 0 = English

\def\courseName{Advanced Algorithms}

\def\coursePrerequisites{\begin{itemize} \item Progettazione degli Algoritmi \end{itemize}}

% \def\book{TODO}

% \def\authorName{Simone Bianco}
% \def\email{bianco.simone@outlook.it}
% \def\github{https://github.com/Exyss/university-notes}
% \def\linkedin{https://www.linkedin.com/in/simone-bianco}

\def\authorName{Alessio Bandiera}
\def\email{alessio.bandiera02@gmail.com}
\def\github{https://github.com/aflaag-notes}
\def\linkedin{https://www.linkedin.com/in/alessio-bandiera-a53767223}

%%%%%%%%%%%%
% Packages %
%%%%%%%%%%%%

\usepackage{../../packages/Nyx/nyx-packages}
\usepackage{../../packages/Nyx/nyx-styles}
\usepackage{../../packages/Nyx/nyx-frames}
\usepackage{../../packages/Nyx/nyx-macros}
\usepackage{../../packages/Nyx/nyx-title}
\usepackage{../../packages/Nyx/nyx-intro}

%%%%%%%%%%%%%%
% Title-page %
%%%%%%%%%%%%%%

\logo{../../packages/Nyx/logo.png}

\if\useItalian1
    \institute{\curlyquotes{\hspace{0.25mm}Sapienza} UniversitÃ  di Roma}
    \faculty{Ingegneria dell'Informazione,\\Informatica e Statistica}
    \department{Dipartimento di Informatica}
    \ifdefined\book
        \subtitle{Appunti integrati con il libro \book}
    \fi
    \author{\textit{Autore}\\\authorName}
\else
    \institute{\curlyquotes{\hspace{0.25mm}Sapienza} University of Rome}
    \faculty{Faculty of Information Engineering,\\Informatics and Statistics}
    \department{Department of Computer Science}
    \ifdefined\book
        \subtitle{Lecture notes integrated with the book \book}
    \fi
    \author{\textit{Author}\\\authorName}
\fi


\title{\courseName}
\date{\today}

% \supervisor{Linus \textsc{Torvalds}}
% \context{Well, I was bored\ldots}

\addbibresource{./references.bib}

%%%%%%%%%%%%
% Document %
%%%%%%%%%%%%

\begin{document}
    \maketitle

    % The following style changes are valid only inside this scope 
    {
        \hypersetup{allcolors=black}
        \fancypagestyle{plain}{%
        \fancyhead{}        % clear all header fields
        \fancyfoot{}        % clear all header fields
        \fancyfoot[C]{\thepage}
        \renewcommand{\headrulewidth}{0pt}
        \renewcommand{\footrulewidth}{0pt}}

        \romantableofcontents
    }

    \introduction

    %%%%%%%%%%%%%%%%%%%%%

    \chapter{Approximation algorithms}

    TODO \todo{missing introduction}

    Lastly, we will adopt the following convention in these notes, when talking about \tbf{approximation algorithm}. Given a problem $P$ for which there is an optimal solution OPT, and an algorithm $A$ that solves $P$ which outputs a solution ALG to $P$

    \begin{itemize}
        \item if $P$ is a \tbf{minimization} problem, we say that $A$ yields an $\alpha$-approximation of $P$ if $$\mathrm{ALG} \le \alpha \cdot \mathrm{OPT}$$
        \item if $P$ is a \tbf{maximization} problem, we say that $A$ yields an $\alpha$-approximation of $P$ if $$\mathrm{ALG} \ge \alpha \cdot \mathrm{OPT}$$
    \end{itemize}

    \section{Approximation through randomness}

    \subsection{The Maximum Cut problem} \label{maxcut}

    The first problem that will be discussed is the \href{https://en.wikipedia.org/wiki/Maximum_cut}{Maximum Cut} problem. The \tbf{Maximum Cut problem} --- in the unweighted case --- is a classic combinatorial optimization problem in the branch of \href{https://en.wikipedia.org/wiki/Graph_theory}{graph theory}, in which we seek to partition the vertices of an undirected graph into two disjoint subsets while maximizing the number of edges that have endpoints in both subsets. More formally, we will define a \tbf{cut} of a graph as follows.

    \begin{frameddefn}{Cut}
        Given an undirected graph $G = (V, E)$, and two subsets $S, T \subseteq V$, the \tbf{cut} induced by $S$ and $T$ on $G$ is defined as follows $$\mathrm{cut}(S, T) = \{e \in E : \abs{S \cap e} = \abs{T \cap e} = 1\}$$ In the case in which $T = \overline S = V - S$ we will simply write $\mathrm{cut}(S) = \mathrm{cut}(S, \overline S)$.
    \end{frameddefn}

    Note that in the definition above we are defining the cut of a graph through the intersection between a set of vertices and edges in $E$; this is because, in the undirected case, we will consider the edges of a graph $G = (V, E)$ as sets of 2 elements $$E = \{\{u, v\} \mid u, v \in V\}$$ Therefore, given two sets of vertice $S$ and $T$, the cut induced by $S$ and $T$ is simply the set of edges that have only one endpoint in $S$ and the other one in $T$.

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[-,>=stealth,shorten >=1pt,auto,node distance=1.3cm, thick,main node/.style={scale=0.9,circle,draw,font=\sffamily\normalsize}]

            \node[circle, draw]  (1) []{};
            \node[circle, draw, fill=red]  (2) [below of = 1]{};
            \node[circle, draw, fill=red]  (3) [right of = 1]{};
            \node[circle, draw, fill=red]  (4) [below of = 3]{};

            % \path[every node/.style={font=\sffamily\small}]

            \draw[-] (1) [red] to (2);
            \draw[-] (1) [red] to (3);
            \draw[-] (2) to (4);
            \draw[-] (3) to (4);

            ;
        \end{tikzpicture}
        \caption{Given the set of red vertices $S$, the green edges represent $\mathrm{cut}(S)$.}
    \end{figure}

    With this definition, we can introduce the \tbf{Maximum Cut} problem, which is defined as follows.

    \begin{frameddefn}{Maximum Cut problem}
        The \tbf{Maximum Cut} (MC) problem is defined as follows: given an undirected graph $G = (V, E)$, determine the set $S \subseteq V$ that maximizes $\abs{\mathrm{cut}(S)}$.
    \end{frameddefn}

    Although this problem is known to be \textsf{APX-Hard} \cite{maxcut}, approximation algorithms and heuristic methods like greedy algorithms and local search are commonly used to find near-optimal solutions.

    For now, we present the following \tbf{randomized algorithm}, which provides a straightforward $\frac{1}{2}$-approximation for MC. This algorithm runs in polynomial time and achieves the approximation guarantee with high probability.

    \begin{framedalgo}[label={max cut alg}]{Random Cut}
        Given an undirected graph $G = (V, E)$, the algorithm returns a cut of $G$. \\
        \hrule

        \quad
        \begin{algorithmic}[1]
            \Function{RandomCut}{$G$}
                \State $S := \varnothing$
                \For{$v \in V$}
                    \State Let $c_v$ be the outcome of the flip of an independent fair coin
                    \If{$c_v == \mathrm H$}
                        \State $S = S \cup \{v\}$
                    \EndIf
                \EndFor
                \State \textbf{return} $S$
            \EndFunction
        \end{algorithmic}
    \end{framedalgo}

    Note that this algorithm is powerful, because \tit{it does not care about the structure of the graph in input}, since the output is completely determined by the coin flips performed in the \texttt{for} loop. Now we will prove that this algorithm provides a correct \tbf{expected $\frac{1}{2}$-approximation} of MC.
    
    \begin{framedthm}[label={expected random cut}]{Expected approximation ratio of \textsc{RandomCut}}
        Let $G= (V, E)$ be a graph, and let $S^*$ be an optimal solution to MC on $G$. Then, given $S = \textsc{RandomCut}(G)$, it holds that $$\mathbb E [\abs{\mathrm{cut}(S)}] \ge \dfrac{1}{2} \abs{\mathrm{cut}(S^*)}$$
    \end{framedthm}

    \begin{proof}
        By definition, note that $$\forall e \in E \quad e \in \mathrm{cut}(S) \iff \abs{S \cap e} = 1$$ Consider an edge $e = \{v, w\} \in E$; then, by definition $$\{v, w\} \in \mathrm{cut}(S) \iff (v \in S \land w \notin S) \lor (v \notin S \land w \in S)$$ and let $\xi_1$ and $\xi_2$ be these last two events respectively. Then $$\Pr[\xi_1] = \Pr[c_v = \mathrm{heads} \land c_w = \mathrm{tails}]$$ by definition of the algorithm, and by independence of the flips of the fair coins we have that $$\Pr[\xi_1] = \Pr[c_v = \mathrm{heads}] \cdot \Pr[c_w = \mathrm{tails}] = \dfrac{1}{2} \cdot \dfrac{1}{2} = \dfrac{1}{4}$$ Analogously, we can show that $$\Pr[\xi_2] = \dfrac{1}{4}$$ This implies that $$\Pr[e \in \mathrm{cut}(S)] = \Pr[\xi_1 \lor \xi_2] = \Pr[\xi_1] + \Pr[\xi_2] - \Pr[\xi_1 \land \xi_2] = \dfrac{1}{4} + \dfrac{1}{4} - 0 = \dfrac{1}{2}$$ Hence, we have that $$\mathbb E [\abs{\mathrm{cut}(S)}] = \sum_{e \in E}{1 \cdot \Pr[e \in \mathrm{cut}(S)]} = \dfrac{\abs E}{2} \ge \dfrac{\abs{\mathrm{cut}(S^*)}}{2}$$ where the last inequality directly follows from the definition of cut of a graph.
    \end{proof}

    As previously mentioned, this algorithm has an \tbf{expected approximation ratio} of $\frac{1}{2}$, which implies that it may return very bad solutions in some cases, depending on the outcomes of the coin flips. However, thanks to the following algorithm, we can actually transform the \tbf{guarantee of expectations} into a \tbf{guarantee of high probability} --- note that it is possible to show that the previous algorithm provides guarantee of high probability as well, but the proof is much more complex.

    \begin{framedalgo}{$t$-times Random Cut}
        Given an undirected graph $G = (V, E)$ and an integer $t > 0$, the algorithm returns a cut of $G$. \\
        \hrule

        \quad
        \begin{algorithmic}[1]
            \Function{$t$-timesRandomCut}{$G$, $t$}
                \For{$i \in [t]$}
                    \State $S_i := \textsc{RandomCut}(G)$
                \EndFor
                \State \textbf{return} $S \in \argmax_{i \in [t]}{\abs{\mathrm{cut}(S_i)}}$
            \EndFunction
        \end{algorithmic}
    \end{framedalgo}

    The algorithm above simply runs the \textsc{RandomCut} algorithm $t$ times, and returns the set $S_i$ that maximizes the cut, among all the various $S_1, \ldots, S_t$. The following theorem will show that a \tit{reasonable number} of runs of the \textsc{RandomCut} algorithm suffices in order to almost certainly obtain a $\approx \frac{1}{2}$-approximation of any optimal solution.

    \begin{framedthm}{}
        Let $G= (V, E)$ be a graph, and let $S^*$ be an optimal solution to MC on $G$. Then, given $S = \textsc{$t$-timesRandomCut}(G, t)$, it holds that $$\Pr \sbk{\abs{\mathrm{cut}(S)} > \dfrac{1 - \varepsilon}{2} \abs{\mathrm{cut}(S^*)}} > 1 - \delta$$ where $t = \frac{2}{\varepsilon} \ln {\frac{1}{\delta}}$ and $0 < \varepsilon, \delta < 1$.
    \end{framedthm}
    
    \begin{proof}
        For each $i \in [t]$, let $C_i := \abs{\mathrm{cut}(S_i)}$ for each $S_i$ defined by the algorithm, and let $N_i := \abs E - C_i$. Let $0 < \varepsilon < 1$; since $N_i$ is a non-negative random variable, by \href{https://en.wikipedia.org/wiki/Markov%27s_inequality}{Markov's inequality} we have that $$\Pr[N_i \ge (1 + \varepsilon) \mathbb E [N_i]] \le \dfrac{1}{1 + \varepsilon} = 1 - \dfrac{\varepsilon}{1 + \varepsilon} \le 1 - \dfrac{\varepsilon}{2}$$ In particular, this inequality can be rewritten as follows:
        \begin{equation*}
            \begin{split}
                1 - \dfrac{\varepsilon}{2} &\ge \Pr[N_i \ge (1 + \varepsilon) \mathbb E [N_i]] \\
                                           &= \Pr[\abs E - C_i \ge (1 + \varepsilon)(\abs E - \mathbb E [C_i])] \\
                                           &= \Pr[- \varepsilon \abs E \ge C_i - (1 + \varepsilon) \mathbb E [C_i]]
            \end{split}
        \end{equation*}
        As shown in the proof of \cref{expected random cut}, we know that $\mathbb E [C_i] = \frac{\abs E}{2}$, therefore
        \begin{equation*}
            \begin{split}
                1 - \dfrac{\varepsilon}{2} &\ge \Pr[- \varepsilon \abs E \ge C_i - (1 + \varepsilon) \mathbb E [C_i]] \\
                                           &= \Pr \sbk{- \varepsilon \abs E \ge C_i - \dfrac{1 + \varepsilon}{2} \abs E} \\
                                           &= \Pr \sbk{- \varepsilon \dfrac{\abs E}{2} \ge C_i - \dfrac{\abs E}{2}} \\
                                           &= \Pr \sbk{\dfrac{1 - \varepsilon}{2} \abs E\ge C_i} \\
                                           &= \Pr \sbk{(1 - \varepsilon) \mathbb E[C_i] \ge C_i} \\
            \end{split}
        \end{equation*}
        Note that the event in the last probability, namely $$\abs{\mathrm{cut}(S_i)}  \le (1 - \varepsilon) \mathbb E [\abs{\mathrm{cut}(S_i)}]$$ corresponds to a \curlyquotes{bad} solution, i.e. one whose cardinality is at most $(1 - \varepsilon)$-th of the expected value.

        By definition of the algorithm, each of the $t$ runs of the \textsc{RandomCut} algorithm is independent from the others, therefore the probability of \tit{all} the solutions $S_1, \ldots, S_t$ being \curlyquotes{bad} is bounded by $$\Pr[\forall i \in [t]  \quad C_i \le (1 - \varepsilon) \mathbb E [C_i]] = \prod_{i = 1}^t{\Pr[C_i \le (1 - \varepsilon)\mathbb E[C_i]]} \le \rbk{1 - \dfrac{\varepsilon}{2}}^t$$

        Using the fact that $$\forall x \in \R \quad 1 - x \le e^{-x} \implies 1 - \frac{\varepsilon}{2} \le e^{- \frac{\varepsilon}{2}}$$ we have that $$\Pr[\forall i \in [t] \quad C_i \le (1 - \varepsilon) \mathbb E[C_i]] \le \rbk{1 - \dfrac{\varepsilon}{2}}^t \le e^{- \frac{\varepsilon}{2} \cdot t} = e^{- \ln {\frac{1}{\delta}}} = \delta$$

        Therefore, the probability that at least one among $S_1, \ldots, S_t$ is a \curlyquotes{good} solution is bounded by $$\Pr[\exists i \in [t] \quad C_i > (1 - \varepsilon) \mathbb E [C_i]] = 1 - \Pr[\forall i \in [t] \quad C_i \le (1 - \varepsilon) \mathbb E [C_i]] \ge 1 - \delta$$

        placeholder \todo{last part}
    \end{proof}

    Note that this result is \tit{very powerful}: for instance, if $\varepsilon = \delta = 0.1$, we get that $$\Pr[\abs{\mathrm{cut}(S)} > 0.45 \cdot \abs{\mathrm{cut}(S^*)}] \ge 0.9$$ and $t \approx 46$, meaning that we just need to run the \textsc{RandomCut} algorithm approximately 46 times in order to get a solution that is better than a $0.45$-approximation with $90\%$ probability.

    \section{Approximation through reduction}

    \subsection{The Vertex Cover problem}

    Another very important problem in graph theory is the \href{https://en.wikipedia.org/wiki/Vertex_cover}{Vertex Cover} problem, which concerns the combinatorial structure of the \tbf{vertex cover}, defined as follows.

    \begin{frameddefn}{Vertex cover}
        Given an undirected graph $G = (V, E)$, a \tbf{vertex cover} of $G$ is a set of vertices $S \subseteq V$ such that $$\forall e \in E \quad \exists v \in S \quad v \in e$$
    \end{frameddefn}

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[-,>=stealth,shorten >=1pt,auto,node distance=1.3cm, thick,main node/.style={scale=0.9,circle,draw,font=\sffamily\normalsize}]

            \node[circle, draw, fill=red] (1) []{};
            \node[circle, draw, fill=red] (2) [right of = 1]{};
            \node[circle, draw, fill=red] (3) [below of = 1]{};
            \node[circle, draw, fill=red] (4) [below of = 2]{};
            \node[circle, draw, fill=red] (5) [left of = 1]{};
            \node[circle, draw, ] (6) [right of = 2]{};

            \draw[-] (1) to (2);
            \draw[-] (1) to (3);
            \draw[-] (2) to (4);
            \draw[-] (1) to (5);
            \draw[-] (3) to (4);
            \draw[-] (3) to (5);
            \draw[-] (2) to (6);

            ;
        \end{tikzpicture}
        \caption{An example of a vertex cover.}
    \end{figure}

    As shown in figure, a vertex cover is simply a set of vertices that must \tit{cover} all the edges of the graph. Clearly, the trivial vertex cover is representd by $S = V$, but a more interesting solution to the problem is represented by the \tbf{minimum vertex cover}.

    \begin{frameddefn}{Vertex Cover problem}
        The \tbf{Vertex Cover} (VC) problem is defined as follows: given an undirected graph $G = (V, E)$, determine the vertex cover $S \subseteq V$ of smallest cardinality.
    \end{frameddefn}

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[-,>=stealth,shorten >=1pt,auto,node distance=1.3cm, thick,main node/.style={scale=0.9,circle,draw,font=\sffamily\normalsize}]

            \node[circle, draw, fill=red] (1) []{};
            \node[circle, draw, fill=red] (2) [right of = 1]{};
            \node[circle, draw, fill=red] (3) [below of = 1]{};
            \node[circle, draw, ] (4) [below of = 2]{};
            \node[circle, draw, ] (5) [left of = 1]{};
            \node[circle, draw, ] (6) [right of = 2]{};

            \draw[-] (1) to (2);
            \draw[-] (1) to (3);
            \draw[-] (2) to (4);
            \draw[-] (1) to (5);
            \draw[-] (3) to (4);
            \draw[-] (3) to (5);
            \draw[-] (2) to (6);

            ;
        \end{tikzpicture}
        \caption{This is the \tit{minimum vertex cover} of the previous graph.}
    \end{figure}

    As famously proved by \textcite{karp} in 1972, this problem is \NPclass-Complete, hence we are interested in finding algorithms that allow to find approximations of optimal solutions. For instance, an algorithm that is able to approximate VC concerns the \href{https://en.wikipedia.org/wiki/Matching_(graph_theory)}{matching} problem.

    \begin{frameddefn}{Matching}
        Given an undirected graph $G = (V, E)$, a \tbf{matching} of $G$ is a set of edges $A \subseteq E$ such that $$\forall e, e' \in A \quad e \cap e' = \varnothing$$
    \end{frameddefn}

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[-,>=stealth,shorten >=1pt,auto,node distance=1.3cm, thick,main node/.style={scale=0.9,circle,draw,font=\sffamily\normalsize}]

            \node[circle, draw] (1) []{};
            \node[circle, draw] (2) [right of = 1]{};
            \node[circle, draw] (3) [below of = 1]{};
            \node[circle, draw] (4) [below of = 2]{};
            \node[circle, draw] (5) [left of = 1]{};
            \node[circle, draw] (6) [right of = 2]{};

            \draw[-] (1) [red] to (2);
            \draw[-] (1) to (3);
            \draw[-] (2) to (4);
            \draw[-] (1) to (5);
            \draw[-] (3) to (4);
            \draw[-] (3) [red] to (5);
            \draw[-] (2) to (6);

            ;
        \end{tikzpicture}
        \caption{A \tit{matching} of the previous graph.}
        \label{matching}
    \end{figure}

    As shown in figure, a matching is nothing more than a set of edges that must not share endpoints with each other --- for this reason, in literature it is often referred to as \tbf{independent edge set}. Differently from the vertex cover structure, in this context the trivial matching is clearly the set $A = \varnothing$, which vacuously satisfies the matching condition. However, a more interesting solution is represented by the \tbf{maximum matching}, but this time we have to distinguish two slightly different definitions, namely the concept of \tit{maximal} and \tit{maximum}.

    \begin{frameddefn}{Maximal matching}
        A \tbf{maximal matching} is a matching that cannot be extended any further.
    \end{frameddefn}
    
    For instance, the matching shown in \cref{matching} is actually a \tbf{maximal matching}, because no other edge in $E$ can be added to the current set of edges $A$ of the matching without breaking the matching condition.

    \begin{frameddefn}{Maximum matching}
        A \tbf{maximum matching} is a matching that has the largest cardinality.
    \end{frameddefn}
    
    Clearly, the previous example does not represent a \tbf{maximum matching}, because the following set of edges

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[-,>=stealth,shorten >=1pt,auto,node distance=1.3cm, thick,main node/.style={scale=0.9,circle,draw,font=\sffamily\normalsize}]

            \node[circle, draw] (1) []{};
            \node[circle, draw] (2) [right of = 1]{};
            \node[circle, draw] (3) [below of = 1]{};
            \node[circle, draw] (4) [below of = 2]{};
            \node[circle, draw] (5) [left of = 1]{};
            \node[circle, draw] (6) [right of = 2]{};

            \draw[-] (1) to (2);
            \draw[-] (1) to (3);
            \draw[-] (2) to (4);
            \draw[-] (1) [red] to (5);
            \draw[-] (3) [red] to (4);
            \draw[-] (3) to (5);
            \draw[-] (2) [red] to (6);

            ;
        \end{tikzpicture}
    \end{figure}

    is still a valid matching for the graph, but has a larger cardinality than the previous set.

    Differently from VC, a \tit{maximum matching} can be found in polynomial time. Moreover, the following algorithm can be used to determine a \tit{maximal matching} of a given graph.

    \begin{framedalgo}{Maximal matching}
        Given an undirected graph $G = (V, E)$, the algorithm returns a maximal matching of $G$. \\
        \hrule

        \quad
        \begin{algorithmic}[1]
            \Function{MaximalMatching}{$G$}
                \State $S := \varnothing$
                \While{$E \neq \varnothing$}
                    \State Choose $e\in E$
                    \State $S = S \cup \{e\}$
                    \State Remove from $E$ all the edges incident on $u$ or on $v$
                    \State $E = E - \{e\}$
                \EndWhile
                \State \textbf{return} $S$
            \EndFunction
        \end{algorithmic}
    \end{framedalgo}

    \idea{
        The algorithm is very straightforward: at each iteration, a random edge $e = \{u, v\}$ is chosen from $E$, and then any edge $e' \in E$ such that $e \cap e' \neq \varnothing$ is removed from $E$.

        Clearly, line 6 ensures that the output is a \tit{matching}, and the terminating condition of the \texttt{while} loop ensures that it is \tit{maximal}, but since the output depends on the chosen edges, $S$ is not guaranteed to be \tit{maximum}.
    }

    Another major reason on why we focus on matchings is the following theorem.

    \begin{framedthm}{Matchings bound vertex covers}
        Given an undirected graph $G = (V, E)$, a matching $A \subseteq E$ of $G$, and a vertex cover $S \subseteq V$ of $G$, we have that $\abs S \ge \abs A$.
    \end{framedthm}

    \begin{proof}
        By definition, any vertex cover $S$ of $G = (V, E)$ is also a vertex cover for $G^B = (V, B)$, for any set of edges $B \subseteq E$, and in particular this is true for $G^A = (V, A)$.

        Now consider $G^A$, and a vertex cover $C$ on it: by construction we have that $\Delta \le 1$, therefore any vertex in $C$ will cover at most 1 edge of $A$. This implies that if $\abs C = k$, then $C$ will cover at most $k$ edges of $G^A$.

        Lastly, since $G^A$ has $\abs A$ edges by definition, any vertex cover defined on $G^A$ has to contain at least $\abs A$ vertices. This implies that no vertex cover $S$ of $G$ smaller than $\abs A$ can exist, because $S$ will have to cover at least the edges in $A$.
    \end{proof}
    
    Thanks to this theorem, we can easily show that the algorithm that we just presented in order to find maximal matchings is a 2-approximation of VC. 

    \begin{framedthm}{2-approximation of VC problem}
        Given a graph $G$, and $M = \textsc{MaximalMatching}(G)$, let $S := \bigcup_{e \in M}{e}$. Then $S$ is a 2-approximation of VC on $G$.
    \end{framedthm}

    \begin{proof}
        Consider an optimal solution $S^*$ to VC on $G$, and let $M = \{e_1, \ldots, e_t\}$. Note that at each iteration of the algorithm exactly 1 edge is added to $M \subseteq V$, hence it always holds that $$S_i \cap S_{i + 1} = e_i = \{u, v\}$$ for any iteration $i \in [t - 1]$. Moreover, since $M$ is a matching, it holds that $\forall e, e' \in M \quad e \cap e' = \varnothing$, therefore $\abs{S} = 2 \abs{M}$. Finally, by the previous theorem we have that $\abs{M} \le \abs{S^*}$, concluding that $$\abs S = 2 \abs M \le 2 \abs{S^*}$$
    \end{proof}
    
    This 2-approximation algorithm is conjectured to be optimal, but it has not been proven yet. In fact, VC is conjectured to be \NPclass-Hard to $(2 - \varepsilon)$-approximate, for any $\varepsilon > 0$.

    Interestingly, the decisional version of VC is \href{https://en.wikipedia.org/wiki/Parameterized_complexity#FPT}{Fixed Parameter Tractable}. This characterization comes from the nature of the problem: for each edge $e = \{u, v\}$ of a given undirected graph $G = (V, E)$, either $u$ or $v$ has to be in the vertex cover, therefore it possible to approach VC by trying all possible choices of set of vertices $S \subseteq V$, and backtrack if necessary. The following algorithm employs this idea.

    \begin{framedalgo}{Decisional VC}
        Given an undirected graph $G = (V, E)$, and an integer $k$, the algorithm returns \texttt{True} if and only if $G$ admits a vertex cover of size $k$. \\
        \hrule

        \quad
        \begin{algorithmic}[1]
            \Function{VC}{$G$, $k$}
                \If{$E == \varnothing$}
                    \State \tbf{return} \texttt{True}
                \ElsIf{$k == 0$}
                    \State \tbf{return} \texttt{False}
                \Else
                    \State Choose $e = \{u, v\} \in E$
                    \If{$\textsc{VC}(G[V - \{u\}]), k - 1)$}
                        \State \tbf{return} \texttt{True}
                    \EndIf
                    \If{$\textsc{VC}(G[V - \{v\}]), k - 1)$}
                        \State \tbf{return} \texttt{True}
                    \EndIf
                    \State \tbf{return} \texttt{False}
                \EndIf
            \EndFunction
        \end{algorithmic}
    \end{framedalgo}

    Note that this algorithm actually solves the \tit{decisional} version of VC. Moreover, the algorithm uses the definition of \tbf{induced subgraph}, which is the following.

    \begin{frameddefn}{Induced subgraph}
        Given a graph $G = (V, E)$, and a set of vertices $S \subseteq V$, then $G[S]$ represents the \tbf{subgraph induced by $S$ on $G$}, obtained by removing from $G$ all the nodes of $V - S$ and the edges incident on them.
    \end{frameddefn}

     \begin{figure}[H]
        \centering

        \begin{tabular}{ccc}
            \begin{tikzpicture}[-,>=stealth,shorten >=1pt,auto,node distance=1.3cm, thick,main node/.style={scale=0.9,circle,draw,font=\sffamily\normalsize}]

                \node[circle, draw, fill=red] (1) []{};
                \node[circle, draw, fill=red] (2) [right of = 1]{};
                \node[circle, draw, fill=red] (3) [below of = 1]{};
                \node[circle, draw, fill=red] (4) [below of = 2]{};
                \node[circle, draw] (5) [left of = 1]{};
                \node[circle, draw] (6) [right of = 2]{};

                \draw[-] (1) to (2);
                \draw[-] (1) to (3);
                \draw[-] (2) to (4);
                \draw[-] (1) to (5);
                \draw[-] (3) to (4);
                \draw[-] (3) to (5);
                \draw[-] (2) to (6);

                ;
            \end{tikzpicture}

            &\qquad\qquad&

            \begin{tikzpicture}[-,>=stealth,shorten >=1pt,auto,node distance=1.3cm, thick,main node/.style={scale=0.9,circle,draw,font=\sffamily\normalsize}]

                \node[circle, draw] (1) []{};
                \node[circle, draw] (2) [right of = 1]{};
                \node[circle, draw] (3) [below of = 1]{};
                \node[circle, draw] (4) [below of = 2]{};

                \draw[-] (1) to (2);
                \draw[-] (1) to (3);
                \draw[-] (2) to (4);
                \draw[-] (3) to (4);

                ;
            \end{tikzpicture}
        \end{tabular}

        \caption{On the left: a graph $G$ and a set of vertices $S$. On the right: the graph $G[S]$.}
    \end{figure}

    \idea{
        The structure of the algorithm consists of a simple backtracking algorithm:
        \begin{itemize}
            \item if the current graph has no edges, we covered every edge of the graph, therefore we return \texttt{True}
            \item if the current graph has some edges, but $k = 0$, then $G$ does not admit a vertex cover of size $k$, thus we return \texttt{False}
            \item if the current graph has some edges, and $k \neq 0$, then we choose an edge $e = \{u, v\} \in E$ arbitrarily, and we try to consider first $u$ then $v$ in a possible vertex cover --- note that $G[V - \{x\}]$ is a graph that does not contain $x$, neither any edge adjacent to it; if both attempts fail, we return \texttt{False}
        \end{itemize}
    }

    \cost{
        It is easy to see that the cost directly depends on the number of recursive calls that the algorithm performs, which is $2^k$ in the worst case, and the cost of constructing $G[V - \{x\}]$, which we can assume to be $O\rbk{n^2}$. Hence, the algorithm has a total cost of $O \rbk{2^k \cdot n^2}$.
    }

    \section{Exercises}

    \begin{framedprob}{}
        Does there exist a polytime algorithm that, given a graph $G = (V, E)$, returns \texttt{True} if and only if the maximum cut of $G$ has size $\abs E$?
    \end{framedprob}

    \solution{
        If $G$ is bipartite, clearly the maximum cut of $G$ has size $\abs E$ --- it suffices to consider the cut induced by one of the two independent sets of $G$.

        Viceversa, suppose that $G$ is not bipartite; a well-known result in graph theory shows that $G$ is not bipartite if and only if $G$ contains an odd-length cycle. Hence, let $C$ be an odd-length cycle of $G$; it is easy to see that it is not possible to include all the edges of $C$ inside the same cut, no matter the set of vertices we choose --- in fact, even if we consider alternating vertices in our set $S$, since $\abs {V(C)}$ is odd we will end up with an edge $uv \in E(C)$ such that either $u, v \in S$ or $u, v \notin S$, meaning that $uv \notin \mathrm{cut}(S)$.

        This proves that $G$ is bipartite if and only if the maximum cut of $G$ has size $\abs{E(G)}$, meaning that a simple DFS over $G$ suffices to provide a polytime algorithm that solves the problem.
    }

    \begin{framedprob}{}
        Let $G = (V, E)$ be a graph having a vertex cover of size $k$. Show that the vertices of $G$ can be colored with $k + 1$ colors in such a way that, for each $uv \in E$, $u$ and $v$ have different colors.
    \end{framedprob}

    \solution{
        Let $S$ be the vertex cover of $G$ of size $k$ and consider a coloring function $f$ such that:

        \begin{itemize}
            \item each vertex in $S$ is assigned a different color in $[k]$
            \item any vertex in $V - S$ is assigned $k + 1$
        \end{itemize}

        By way of contradiction, suppose that there is an edge $uv \in E$ such that$f(u) = f(v)$; since the vertices in $S$ all have different colors, and all the vertices outside $S$ are assigned $k + 1$ --- which is not in $[k]$ --- it must be that $f(u) = f(v) = k + 1$. However, if this is the case, it means that $uv$ has both endpoints outside $S$, meaning that $S$ is not a vertex cover of $G$ $\lightning$.
    }

    \chapter{Mathematical Programming}

    Up to this point, we have introduced several \NPclass-Complete problems and discussed approximation algorithms that provide near-optimal solutions within a certain approximation ratio. We will now explore a new technique that can be leveraged to design approximation algorithms for a broader class of problems, namely \tbf{mathematical programming}.

    \href{https://en.wikipedia.org/wiki/Mathematical_optimization}{Mathematical programming} is a powerful framework used to model and solve \tbf{optimization problems} across various fields, including operations research, computer science and engineering. It provides a structured way to \tit{maximize} or \tit{minimize} an \tbf{objective function} while satisfying a set of \tbf{constraints}. In particular, optimization problems are usually expressed as follows

    \begin{figure}[H]
        \centering
        \[\begin{array}{ccl}
            \qquad\qquad\quad
            & \max \; f(x) \\\\
            & g_i(x) \le b_i & \forall i \in [n] \\
            & x \in V
        \end{array}\]
    \end{figure}

    In this example, we see that

    \begin{itemize}
        \item $x$ is a \tit{vector} that lies inside the vector space $V$
        \item the objective function is $f(x)$, which can be either maximized or minimized
        \item $g_i(x) \le b_i$ is a constraint --- i.e. an inequality --- that $x$ must satisfy
    \end{itemize}

    Among the most widely studied forms of mathematical programming are \tbf{Linear Programming} (LP), \tbf{Integer Programming} (IP), and \tbf{Semidefinite Programming} (SDP), each with distinct properties and applications.

    Starting from linear programming, in LPs both the objective function and the constraints are \tit{linear} w.r.t. $V$, and there are no \tit{equality} constraints.

    \begin{figure}[H]
        \centering
        \[\begin{array}{ccl}
            \qquad\qquad\quad
            & \max \; x_1 + x_2 \\\\
            & 2x_1 + x_2 \le 10 \\
            & x_i \ge 0 & \forall i \in [n]\\
            & x \in \R^n
        \end{array}\]
        \caption{Example of an LP.}
    \end{figure}
   
    LPs can be solved in \tbf{polynomial time}: specifically, if an LP has $n$ variables, $m$ constraints, and each coefficient can be expressed as the ratio of two $t$-bit integers (where real numbers are approximated as rationals), then the \href{https://en.wikipedia.org/wiki/Ellipsoid_method}{Ellipsoid method} can solve it in time $O((nmt)^c)$ for some constant $c > 0$. This result extends, to some extent, to SDPs as well.

    Despite its theoretical guarantee of polynomial runtime, in practice the \href{https://en.wikipedia.org/wiki/Simplex_algorithm}{Simplex method} is often preferred, as it operates based on \tit{pivot rules}. In fact, although all known pivot rules for the Simplex method exhibit a theoretical \tit{exponential} lower bound due to specially constructed worst-case instances, its average performance is significantly better than that of the Ellipsoid method in real-world scenarios.

    \section{IPs and LP relaxation}

    Differently from LPs, in IPs the vector space of interest is $\{0, 1\}^n$. IPs can be used to solve a wide range of problems. For instance, given a graph $G =(V, E)$, the \tbf{vertex cover} problem that we discussed in the previous section can be formulated through the following IP:

    \begin{figure}[H]
        \centering
        \[\begin{array}{ccl}
            \qquad\qquad\quad
            & \min \; \displaystyle \sum_{u \in V}^n {x_v} \\\\
            & x_u + x_v \ge 1 & \forall \{u, v\} \in E \\
            & x_v \in \{0,1\} & \forall u \in V
        \end{array}\]
        \caption{IP for VC.}
    \end{figure}

    It is fairly straightforward to prove that an optimal solution to this IP yields an optimal solution to VC.

    \begin{framedlem}[label={vc ip}]{}
        Given a graph $G$, if $\{x^*_u\}_{u \in V}$ is an optimal solution to the previous IP, then $$S^* := \{v \in V \mid x^*_v = 1\}$$ is a minimum vertex cover for $G$.
    \end{framedlem}

    \begin{proof}
        Consider an optimal solution $\{x^*_u\}_{u \in V}$ to VC, and define $S^*$ as the set of vertices $v \in V$ such that $x^*_v = 1$. Note that any optimal solution is also a feasible solution, i.e. it satisfies the constraints of the IP.

        Note that the first constraint of the IP forces that for each $\{u, v\} \in E$ the sum between $x^*_u + x^*_v$ is at least one, and the second constraint forces each variable $x_v^*$ to be either 0 or 1. Therefore, together these two constraints imply that for any edge $\{u, v\} \in E$ at least one between $x_u^*$ and $x_v^*$ is 1, and by definition of $S^*$ this means at least one of the endpoints of $\{u, v\}$ is inside $S^*$. We conclude that $S^*$ is indeed a vertex cover for $G$, and by its definition note that $\abs{S^*} = \sum_{u \in V}{x^*_u}$.

        \claim{
            Given a vertex cover $S$ of a graph $G = (V, E)$, there exists a feasible solution $\{x_u\}_{u \in V}$ to the IP having value $\abs S$.
        }{
            Define the solution $\{x_u\}_{u \in V}$ by setting $x_u = 1$ if and only if $u \in S$. Clearly, the value of this solution is indeed $\sum_{u \in V}{x_u} = \abs S$; moreover, by definition of vertex cover, for any edge $\{u, v\} \in E$ at least one between $u$ and $v$ must be in $S$, therefore at least one between $x_u$ and $x_v$ is set to 1, implying that $x_u + x_v \ge 1$ is always satisfied.
        }

        By way of contradiction, suppose that $S^*$ is not a minimum vertex cover. Hence, there must be another vertex cover $S'$ such that $\abs{S'} < \abs{S^*}$. By the previous claim, this implies that there exists a feasible solution $\{x'_u\}_{u \in V}$ for the IP that has value $\abs{S'}$, but then $$\sum_{u \in V}{x'_u} = \abs{S'} < \abs{S^*} = \sum_{u \in V}{x^*_u}$$ which contradicts the optimality of the solution of $\{x^*_u\}_{u \in V}$ for the IP.
    \end{proof}

    In particular, this lemma implies that VC can be reduced to Integer programming, indeed solving IPs is actually \NPclass-Hard \cite{karp}, differently from LPs. This result shows that IPs cannot be used \tit{directly} to obtain perfect solutions, but they are still very useful thanks to \tbf{relaxation}.

    To \tit{relax} an IP, we simply replace the constraint $x \in \{0, 1\}^n$ with $0 \le x \le 1$, transforming the IP into an LP.

    \begin{figure}[H]
        \centering
        \[\begin{array}{ccl}
            \qquad\qquad\quad
            & \min \; \displaystyle \sum_{u \in V}^n {x_v} \\\\
            & x_u + x_v \ge 1 & \forall \{u, v\} \in E \\
            & 0 \le x_v \le 1 & \forall u \in V
        \end{array}\]
        \caption{LP relaxation for the IP of VC.}
    \end{figure}

    But solving this LP is not enough to obtain a meaningful solution: in fact, a real-valued solution for this problem does not directly yield a vertex cover for a given graph. To fix this issue, the optimal solution of the LP relaxation is usually transformed through techniques such as \tbf{rounding}. Intuitively, the simplest possible type of \tit{rounding rule} is the following: given a solution $\{\overline x_u\}_{u \in V}$ to the LP relaxation, to obtain a VC consider the following set $$S := \cbk{v \in V \mid \overline x_v \ge \tfrac{1}{2}}$$ and for VC in particular, we can prove that this rounding rule actually yields a 2-approximation of any optimal solution.

    \begin{framedthm}[label={lp 2-approx vc}]{}
        Given a graph $G = (V, E)$, if $\{\overline x_u\}_{u \in V}$ is an optimal solution to the LP relaxation of the IP for VC, then $$\overline S := \cbk{v \in V \mid \overline x_v \ge \tfrac{1}{2}}$$ is a 2-approximation for VC.
    \end{framedthm}

    \begin{proof}
        Since $\{\overline x_u\}_{u \in V}$ is an optimal solution to the LP relaxation, it must satisfy the first constraint for which $\overline x_u + \overline x_v \ge 1$ for any $\{u, v\} \in E$. Moreover, for the second constraint we have that $\overline x_u \ge 0$ for all $u \in V$, therefore $$\forall \{u, v\} \in E \quad \max(\overline x_u, \overline x_v) \ge \dfrac{\overline x_u + \overline x_v}{2} \ge \dfrac{1}{2}$$ which means that at least one between $\overline x_u$ and $\overline x_v$ is at least $\frac{1}{2}$, implying that the edge $\{u, v\}$ will be covered by at least one of the two endpoints $u$ and $v$, by definition of $\overline S$. This proves that $\overline S$ is a vertex cover of $G$.

        To prove that $\overline S$ is indeed a 2-approximation, we just need to show the following: given a minimum vertex cover $S^*$ of $G$, it holds that $\abs{\overline S} \le 2 \abs{S^*}$. By the claim in \cref{vc ip}, we know that $S^*$ there exists a feasible solution $\{x^*_u\}_{u \in V}$ for the IP, which must be optimal for the IP since $S^*$ is a minimum vertex cover for $G$. Therefore, we have that
            \begin{equation*}
                \begin{split}
                    \abs{\overline S} &= \sum_{v \in \overline S}{1} \\
                                      &\le \sum_{v \in \overline S}{2 \overline x_v} \quad \quad \rbk{v \in \overline S \implies \overline x_v \ge \tfrac{1}{2}}\\
                                      &= 2 \sum_{v \in \overline S}{\overline x_v} \\
                                      &\le 2 \sum_{v \in V}{\overline x_v} \quad \quad \rbk{\overline S \subseteq V \land \overline x_v \ge 0} \\
                                      &\le 2 \sum_{v \in V}{x^*_v} \\
                                      &= 2 \abs{S^*}
                \end{split}
            \end{equation*}
            where the last inequality comes from the fact that the constraints of the LP are \tit{weaker} than the ones of the IP.
    \end{proof}

    However, note that this result should not come a surprise. In fact, consider a graph $G$, an optimal solution to the LP relaxation $\{\overline x_u\}_{u \in V}$ and $\overline S$ defined as previously shown; given an edge $\{u, v\} \in E(G)$, in the worst case we have that $$\overline x_u = \overline x_v = \dfrac{1}{2}$$ which still satisfies both constraints of the LP relaxation, since $\tfrac{1}{2} + \tfrac{1}{2} = 1 \ge 1$. This means that, in the worst case, both $u$ and $v$ end up inside $\overline S$, which gives an intuitive reason to why this LP relaxation indeed yields a 2-approximation solution.

    \section{Integrality gap}

    Consider a problem P, its equivalent IP, and the relative LP relaxation. Given an instance $I \in \mathrm P$ of the problem, we will denote with $\mathrm{IP^*_P}(I)$ and $\mathrm{LP^*_P}(I)$ the optimal values for the IP and the LP of the problem P on the instance $I$ --- we will omit P and $I$ the context is clear enough. Note that, in general, it holds that $\mathrm{LP^*} \le \mathrm{IP^*}$ since the constraints of the LP relaxation are \tit{weaker} than the ones of the IP.

    For example, the inequalities discussed in the proof of \cref{vc ip} could be rewritten as follows
    \begin{equation*}
        \begin{split}
            \abs{\overline S} &= \ldots \\
                              &\le 2 \sum_{v \in V}{\overline x_v} = 2 \mathrm{LP^*} \\
                              &\le 2 \sum_{v \in V}{x^*_v} = 2 \mathrm{IP^*} = 2 \abs{S^*}\\
        \end{split}
    \end{equation*}
    and in particular $\abs{\overline S} \le 2 \mathrm{LP^*} \le 2 \mathrm{IP^*}$. Can we improve this approximation ratio of 2 through LP relaxation? In general, for any $\alpha$ possible approximation ratio, it must hold that $$\alpha \ge \dfrac{\mathrm{IP^*}}{\mathrm{LP^*}}$$ because otherwise $$\alpha < \mathrm{\dfrac{IP^*}{LP^*}} \implies \abs{\overline S} \le \alpha \mathrm{LP^*} < \mathrm{\dfrac{IP^*}{LP^*}} \cdot \mathrm{LP^*} = \mathrm{IP^*}$$ meaning that $\overline S$ would be a solution better than the optimal solution of the IP, which is impossible. We can generalize this concept as follows.

    \begin{frameddefn}{Integrality gap}
        Given a problem P and an instance $I \in \mathrm P$, the \tbf{integrality gap} between $\mathrm{IP^*_P}(I)$ and $\mathrm{LP^*_P}(I)$ is defined as follows $$\mathrm{IG_P}(I) = \dfrac{\mathrm{IP^*_P}(I)}{\mathrm{LP^*_P}(I)}$$ The integrality gap for the problem P is defined as follows $$\mathrm{IG_P} = \sup_{I \in \mathrm P}{\mathrm{IG_P}(I)} = \sup_{I \in \mathrm P}{\dfrac{\mathrm{IP^*_P}(I)}{\mathrm{LP^*_P}(I)}}$$
    \end{frameddefn}

    In fact, through the previous argument, we can derive the following property that \tit{must} hold for any approximation ratio.

    \begin{framedprop}{Limits of LP relaxation}
        Given a problem P for which there is an $\alpha$-approximation algorithm which uses LP relaxation, it holds that

        \begin{itemize}
            \item if P is a \tbf{minimization problem}, then $\alpha \ge \mathrm{IG_P}$
            \item if P is a \tbf{maximixation problem}, then $\alpha \le \mathrm{IG_P}$
        \end{itemize}
    \end{framedprop}

    % This means that to improve this approximation ratio of 2, we would need to find a value $\varepsilon$ for which $$\abs{\overline S} \le (2 - \varepsilon) \mathrm{LP^*} \le (2 - \varepsilon) \mathrm{IP^*}$$

    Now, let us analyze again VC and try to bound $\mathrm{IG_{VC}}$. Consider the following \tit{clique} graph:
    
    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[-,>=stealth,shorten >=1pt,auto,node distance=1.75cm, thick,main node/.style={scale=0.9,circle,draw,font=\sffamily\normalsize}]

            \node[circle, draw] (1) []{$u$};
            \node[circle, draw] (2) [below left of = 1]{$v$};
            \node[circle, draw] (3) [below right of = 1]{$w$};

            \draw[-] (1) to (2);
            \draw[-] (1) to (3);
            \draw[-] (2) to (3);

            ;
        \end{tikzpicture}
        \caption{The graph $K_3$.}
    \end{figure}

    it is easy to see that $$\mathrm{IP^*}(K_3) = 1 + 1 = 2$$ because 1 single node is not sufficient to cover all 3 edges in $E(K_3)$. However, since the values in the solution of the LP can be \tit{real-valued}, the value of an optimal solution for the LP relaxation is actually achieved by setting $$\overline x_u = \overline x_v = \overline x_w = \dfrac{1}{2} \implies \mathrm{LP}^*(K_3) = 3 \cdot \dfrac{1}{2} = \dfrac{3}{2}$$ therefore, by definition of IG we have that $$\exists I \in \mathrm P \quad \mathrm{IG_{VC}}(I) := \dfrac{\mathrm{IP^*_{VC}}(K_3)}{\mathrm{LP^*_{VC}}(K_3)} = \dfrac{2}{\tfrac{3}{2}} = \dfrac{4}{3} \implies \mathrm{IG_{VC}} := \sup_{I \in \mathrm P}{\dfrac{\mathrm{IP^*_{VC}}(I)}{\mathrm{LP^*_{VC}}(I)}} \ge \dfrac{4}{3}$$ 

    Moreover, \cref{lp 2-approx vc} shows that we already know an algorithm that employs LP relaxation which yields a 2-approximation of VC; therefore, this lower bound on $\mathrm{IG_{VC}}$ --- together with the previous proposition --- implies that any possible approximation ratio $\alpha$ on VC must satisfy $$2 \ge \alpha \ge \mathrm{IG_{VC}} \ge \dfrac{4}{3}$$ The following theorem proves that we can actually bound $\mathrm{IG_{VC}}$ tightly.

    \begin{framedthm}{Integrality gap for VC}
        $\mathrm{IG_{VC}} = 2$
    \end{framedthm}

    \begin{proof}
        We already proved that the upper bound is 2, so we just need to prove that the lower bound is 2 as well.

        Consider a clique $K_n$; by the same reasoning presented for the case of $K_3$, a feasible solution for the LP relaxation over this graph would be $$x_1 = \ldots = x_n = \dfrac{1}{2} \implies \mathrm{LP^*}(K_n) \le n \cdot \dfrac{1}{2} = \dfrac{n}{2}$$

        \claim{
            Any minimum vertex cover of $K_n$ has exactly $n - 1$ vertices.
        }{
            Consider a vertex cover $S = \{v_1\}$ for any vertex $v_1 \in V(K_n)$; since $K_n$ is a clique, by definition $\deg(v_1) = n - 1$, therefore $S$ is able to cover only $n - 1$ \tit{uncovered} edges of $K_n$. Now, consider another vertex $v_2 \in V(K_n)$, and add it to $S = \{v_1, v_2\}$; we observe that $\deg(v_2) = n - 1$, but $v_1 \sim v_2$ because $K_n$ is a clique, therefore $v_2$ will be able to cover only $n - 2$ \tit{uncovered} edges of $K_n$. By the same reasoning, each new vertex $v_i \in V(K_n)$ added to $S$ will be able to cover only $n - i$ \tit{uncovered} edges of $K_n$. However, note that $$\abs{E(K_n)} = \binom{n}{2} = \dfrac{n (n - 1)}{2}$$ therefore, to cover all the edges of $K_n$ we need $\abs S$ to satisfy the following inequality $$\sum_{i = 1}^{\abs S}{(n - i)} \ge \dfrac{n (n - 1)}{2} \iff n \abs S - \dfrac{\abs S (\abs S +1)}{2} \ge \dfrac{n (n - 1)}{2}$$ After some calculations, we derive the following inequality $$\abs{S}^2 + \abs S (1 - 2n) + n (n - 1) \le 0$$ which is satisfied for any value $n - 1 \le \abs S \le n$, therefore a vertex cover of cardinality $\abs S = n - 1$ suffices to cover all the edges of $K_n$.
        }

        This claim shows that for any $n$ it holds that $\mathrm{IP^*}(K_n) = n - 1$, therefore we have that $$\mathrm{IG}(K_n) = \dfrac{\mathrm{IP^*}(K_n)}{\mathrm{LP^*}(K_n)} \ge \dfrac{n - 1}{\tfrac{n}{2}}$$ which means that $$\mathrm{IG_{VC}} = \sup_{I \in \mathrm{VC}}{\mathrm{IG_{VC}}(I)} \ge \lim_{n \to +\infty}{\dfrac{n - 1}{\tfrac{n}{2}}} = 2$$
    \end{proof}

    \section{The Set Cover problem}

    The next problem that we will study can be seen as a \tit{generalization} of the VC problem, which is the \href{https://en.wikipedia.org/wiki/Set_cover_problem}{Set Cover} problem, defined as follows.

    \begin{frameddefn}{Set Cover problem}
        The \tbf{Set Cover} (SC) problem is defined as follows: given a \tit{universe} (or \tit{ground}) set $\mathcal U = [n]$, and a collection of sets $C = \{S_1, \ldots, S_m\}$ such that $S_i \subseteq \mathcal U$, determine the smallest sub-collection $S \subseteq C$ such that $\bigcup_{S_j \in S}{S_j} = \mathcal U$.
    \end{frameddefn}

    In other words, we are asked to determine the smallest sub-collection of the given $C$ such that we can still cover the whole universe set $\mathcal U$. For instance, given $\mathcal U = [3]$ and $S_1 = \{1, 2\}$, $S_2 = \{2, 3\}$, $S_3 = \{1, 3\}$, we can cover $\mathcal U$ with just $S = \{S_1, S_2\}$.

    As for VC, in 1972 \cite{karp} proved that SC is \NPclass-Complete as well. Moreover, similarly to what we did for VC, we can convert SC into an IP, as follows.

    \begin{figure}[H]
        \centering
        \[\begin{array}{ccl}
            \qquad\qquad\quad
            & \min \; \displaystyle \sum_{j = 1}^m {x_j} \\\\
            & \sum\limits_{\substack{j \in [m] : \\ i \in S_j}}{x_j} \ge 1 & \forall i \in [n] \\
            & x_j \in \{0, 1\} & \forall j \in [m]
        \end{array}\]
        \caption{IP for SC.}
    \end{figure}

    The first constraint of the IP states that, given an element $i$ in the universe set, at least one of the variables $x_j$, representing the sets $S_j$ which contain $i$, must be set to 1. In other words, we are guaranteeing that all the elements $i \in \mathcal U$ are covered by at least one set of $C$. Lastly, we want to minimize over the size of the sub-collection of $C$, hence the objective function.

    The LP relaxation that we will consider is the same that we defined for VC. However, differently from VC, the \tit{rounding rule} that we applied to obtain an integral solution --- namely by defining $$S = \{v \in V \mid x_v \ge \tfrac{1}{2}\}$$ cannot be applied for this problem. For instance, say that some element $i \in \mathcal U$ is contained in 3 sets $S_1$, $S_2$ and $S_3$; hence, the second constraint forces the solution of the LP to satisfy $$x_1 + x_2 + x_3 \ge 1$$ Nevertheless, by setting $$x_1 = x_2 = x_3 = \dfrac{1}{3}$$ we would get a feasible solution for the LP relaxation, but then our rounding rule would return an empty set $S = \varnothing$, which is not a feasible solution for our instance of SC since $i$ would not be covered.

    To fix this issue, we are going to present a \tbf{randomized rounding algorithm}, which surprisingly seem to be the \tit{best} approach to perform rounding on LP relaxation solutions that we have at our disposal.

    \begin{framedalgo}{Randomized rounding for SC}
        Given an instance $(\mathcal U, C)$ of SC, the algorithm returns a set cover $A$ for $\mathcal U$. \\
        \hrule

        \quad
        \begin{algorithmic}[1]
            \Function{RandomizedRoundingSC}{$\mathcal U$, $C$}
                \State $A := \varnothing$
                \State $\{\overline x_j\}_{j \in [m]} := \mathrm{LP_{SC}}(\mathcal U, C)$ \Comment{an optimal soluion on the LP relaxation}
                \For{$k \in \sbk{\ceil{2 \ln n}}$}
                    \For{$j \in [m]$}
                        \State Let $c_{k, j}$ be the outcome of the flip of an ind. coin with H prob. set to $\overline x_j$
                        \If{$c_{k, j} == \mathrm H$}
                            \State $A = A \cup \{S_j\}$
                        \EndIf
                    \EndFor
                \EndFor
            \EndFunction
        \end{algorithmic}
    \end{framedalgo}

    First, we are going to prove that the output $A$ of this algorithm is indeed a set cover, with enough probability.

    \begin{framedlem}{}
        Let $(\mathcal U, C)$ be a SC instance, and let $A = \textsc{RandomizedRoundingSC}(\mathcal U, C)$. Then, it holds that $$\Pr[A \ \mathrm{is \ a \ set \ cover}] \ge 1 - \dfrac{1}{n}$$
    \end{framedlem}

    \begin{proof}
        Each iteration of the outermost \ttt{for} loop will be referred to as \tit{phase}.

        \claim{
            The element $i$ is covered by $A$ in \tit{phase} $k$ with probability at least $1 - \frac{1}{e}$.
        }{
            \begin{equation*}
                \begin{split}
                    \Pr[i \ \mathrm{is \ not \ covered \ in \ \mathit{phase}} \ k] &= \prod_{\substack{j \in [m]: \\ i \in S_j}}{(1 - \overline x_j)} \quad \quad (\mathrm{the \ prob. \ of \ T}) \\
                                                                                   &\le \prod_{\substack{j \in [m] : \\ i \in S_j}}{e^{-\overline x_j}} \quad \quad (1 - x \le e^{-x})  \\
                                                                                   &= e^{\displaystyle - \sum\limits_{\substack{j \in [m]: \\ i \in S_j}}{\overline x_j}} \\
                                                                                   &\le e^{-1} \quad \quad (\mathrm{second \ constraint \ of \ the \ LP}) \\
                                                                                   &=\dfrac{1}{e}
                \end{split}
            \end{equation*}
        }

        \claim{
            The element $i$ is not covered by any set of $A$ with probability at most $\frac{1}{n^2}$.
        }{
            \begin{equation*}
                \begin{split}
                    \Pr[i \ \mathrm{is \ not \ covered \ by \ any \ set \ of} \ A] &= \prod_{k = 1}^{\ceil{2 \ln n}}{\Pr[i \ \mathrm{is \ not \ covered \ in \ \mathit{phase}} \ k]} \\
                                                                                   &\le \prod_{k = 1}^{\ceil{2 \ln n}}{\dfrac{1}{e}} \quad \quad (\mathrm{previous \ claim}) \\
                                                                                   &= e^{- \ceil {2 \ln n }} \\
                                                                                   &\le e^{- 2 \ln n} \\
                                                                                   &= \dfrac{1}{n^2}
                \end{split}
            \end{equation*}
        }

        \claim{
            $A$ is a set cover with probability at least $1 - \frac{1}{n}$.
        }{
            \begin{equation*}
                \begin{split}
                    \Pr[A \ \mathrm{is \ not \ a \ set \ cover}] &= \Pr[\exists i \quad i \ \mathrm{is \ not \ covered \ by \ any \ set \ of} \ A] \\
                                                                 &\le \sum_{i = 1}^n{\Pr[i \ \mathrm{is \ not \ covered \ by \ any \ set \ of} \ A]} \\
                                                                 &\le \sum_{i = 1}^n{\dfrac{1}{n^2}} \quad \quad (\mathrm{previous \ claim}) \\
                                                                 &= \dfrac{n}{n^2} \\
                                                                 &= \dfrac{1}{n}
                \end{split}
            \end{equation*}
        }
    \end{proof}

    Next, we will show that this algorithm yields \tit{on average} a $\ceil{2 \ln n}$-approximation of SC.

    \begin{framedlem}{}
        Let $(\mathcal U, C)$ be a SC instance, and let $A = \textsc{RandomizedRoundingSC}(\mathcal U, C)$. Then, it holds that $$\mathbb E \sbk{\abs A} \le \ceil{2 \ln n} \mathrm{IP}^*$$
    \end{framedlem}

    \begin{proof}
        Fix a \tit{phase} $k$, and let $A_k$ be the collection of sets added to $A$ at \tit{phase} $k$; then it holds that $$\mathbb E \sbk{\abs{A_k}} = \sum_{j = 1}^m {1 \cdot \Pr[S_j \in A_k]} = \sum_{j = 1}^m{\overline x_j} = \mathrm{LP^*}$$ Moreover, since $\displaystyle A = \bigcup_{k \in \ceil{2 \ln n}}{A_k}$, we have that $$\mathbb E \sbk{\abs A} \le \mathbb E \sbk{\sum_{k \in \ceil{2 \ln n}}{\abs{A_k}}} = \sum_{k \in \ceil {2 \ln n}}{\mathbb E \sbk{\abs{A_k}}} = \sum_{k \in \ceil{2 \ln n}}{\mathrm{LP^*}} = \ceil{2 \ln n} \mathrm{LP^*} \le \ceil{2 \ln n} \mathrm{IP^*}$$
    \end{proof}

    TODO \todo{missing something}

    Additionally, note that the algorithm can be modified to get a better bound, by simply replacing $\ceil{2 \ln n}$ with $\ceil{(1 + \varepsilon) \ln n}$, for any $\varepsilon > 0$. In fact, with this modification we would get that $\mathbb E \sbk{\abs A} \le \ceil{(1 + \varepsilon) \ln n} \mathrm{LP^*}$, therefore \todo{missing something}

    With VC we were able to provide a tight bound on $\mathrm{IG_{VC}}$; for SC instead, even though the value of $\mathrm{IG_{SC}}$ is known \cite{ig_sc}, the proof is very complex and we will only show a lower bound.

    \begin{framedthm}{Integrality gap of SC}
        For any $n \in \N$, it holds that $$\dfrac{1}{4 \ln 2} \le \mathrm{IG_{SC}} \le \ceil{\ln n}$$
    \end{framedthm}
    
    \begin{proof}
        We already proved the upper bound, so we just need to prove the lower bound. Furthermore, as shown with VC in the previous section, to provide a lower bound on $\mathrm{IG_{SC}}$ it suffices to show an instance of SC for which the bound holds.

        Let $m \ge 2$ be an even integer, and define the following instance of SC: set $$\mathcal U_m := \{e_A \mid A \subseteq [m] \land \abs A  = \tfrac{m}{2}\} \implies n = \abs{\mathcal U_m}= \binom{m}{\tfrac{m}{2}}$$ and define the collection of sets as follows $$\forall j \in [m] \quad S_j := \{e_A \mid e_A \in \mathcal U_m \land j \in A\}$$ and set $C_m := \{S_1, \ldots, S_m\}$ For example, if $m = 4$ we have that $$\mathcal U_4 := \{e_A \mid A \subseteq \{1, 2, 3, 4\} \land \abs A = \tfrac{4}{2} = 2\} = \{e_{\{1, 2\}}, e_{\{1, 3\}}, e_{\{1, 4\}}, e_{\{2, 3\}}, e_{\{2, 4\}}, e_{\{3, 4\}}\}$$ $$S_1 := \{e_{\{1,2\}}, e_{\{1, 3\}}, e_{\{1,4\}}\}$$ $$S_2 := \{e_{\{1, 2\}}, e_{\{2, 3\}}, e_{\{2,4\}}\}$$ $$S_3 := \{e_{\{1, 3\}}, e_{\{2, 3\}}, e_{\{3, 4\}}\}$$ $$S_4 := \{e_{\{1, 4\}}, e_{\{2, 4\}}, e_{\{3, 4\}}\}$$

        Note that, thanks to \href{https://en.wikipedia.org/wiki/Stirling%27s_approximation}{Stirling's approximation} we know that $$n = \binom{m}{\tfrac{m}{2}} = \Theta \rbk{\dfrac{2^m}{\sqrt m}} \implies m = \log n - \Theta(\log \log n)$$

        \claim{
            $\forall m \ge 2 \ \mathrm{even} \quad \mathrm{LP^*_{SC}}(\mathcal U_m, C_m) \le 2$.
        }{
            Consider the solution $$x_1 = \ldots = x_m = \dfrac{2}{m}$$ for the LP relaxation; clearly, $m \ge 2$ therefore $\forall j \in [m] \quad 0 \le x_j \le 1$, and $$\forall e_A \in \mathcal U \quad \sum_{\substack{j \in [m]: \\ e_A \in S_j}}{x_j} = \sum_{\substack{j \in [m]: \\ e_A \in S_j}}{\dfrac{2}{m}} = \abs A \cdot \dfrac{2}{m} = \dfrac{m}{2} \cdot \dfrac{2}{m} = 1 \ge 1$$ hence this is a feasible solution to the LP, and its value is simply given by $m \cdot \frac{2}{m} = 2$.
        }

        \claim{
            $\forall m \ge 2 \ \mathrm{even} \quad \mathrm{IP^*}(\mathcal U_m, C_m) \ge \tfrac{1}{2} \log n - O(\log \log n)$.
        }{
            By way of contradiction, assume that there exists a sub-collection $S = \{S_{i_1}, \ldots, S_{i_k}\} \subseteq C_m$ with $k \le \tfrac{m}{2}$ that covers $\mathcal U_m$. Consider the following set $$T := [m]- \{i_1, \ldots, i_k\}$$ thus, we have that $$\abs T \ge m - \dfrac{m}{2} = \dfrac{m}{2}$$ hence, we can always define a subset $A \subseteq T$ such that $\abs A = \tfrac{m}{2}$. However, we observe that $$A \subseteq T \implies i_1, \ldots, i_k \notin A \implies e_A \notin S_{i_1} \cup \ldots \cup S_{i_k}$$ hence $e_A$ is not covered by $S$ $\lightning$.

            This means that for any set cover $S$ of $(\mathcal U_m, C_m)$ it must hold that $$\abs S > \dfrac{m}{2} = \dfrac{1}{2} \log n - \Theta(\log \log n) \implies \mathrm{IP^*}(\mathcal U_m, C_m) \ge \dfrac{m}{2} = \dfrac{1}{2} \log n - O(\log \log n)$$
        }

        Finally, from these two claims, it follows that $$\mathrm{IG_{SC}}(\mathcal U_m, C_m) = \dfrac{\mathrm{IP^*}(\mathcal U_m, C_m)}{\mathrm{LP^*}(\mathcal U_m, C_m)} \ge \dfrac{\tfrac{1}{2} \log n - O(\log \log n)}{2} = \dfrac{1}{4} \log n - O(\log \log n)$$ which means that $$\mathrm{IG_{SC}} = \sup_{I \in \mathrm{SC}}{\mathrm{IG_{SC}}(I)} \ge \mathrm{IG_{SC}}(\mathcal U_m, C_m) \ge \dfrac{1}{4} \log n = \dfrac{1}{4 \ln 2} \ln n$$
    \end{proof}

    \section{The Densest Subgraph problem}
    
    Up to this point, we only discussed \NPclass-Complete problems, and various techniquesused to obtain approximate solutions. The next problem that we are going to introduce, instead, can be solved in \tbf{polynomial} time, thanks to linear programming. First, consider the following definition.

    \begin{frameddefn}{Graph density}
        Given a graph $G = (V, E)$ such that $V \neq \varnothing$, its \tbf{density} is defined as follows $$\rho(G) := \dfrac{\abs E}{\abs V}$$
    \end{frameddefn}

    Note that we require $V \neq \varnothing$, but we will omit this detail in the following sections.

    For instance, the density of the graph $G = (V, E)$ below

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[-,>=stealth,shorten >=1pt,auto,node distance=1.3cm, thick,main node/.style={scale=0.9,circle,draw,font=\sffamily\normalsize}]

            \node[circle, draw] (1) []{};
            \node[circle, draw] (2) [right of = 1]{};
            \node[circle, draw] (3) [below of = 1]{};
            \node[circle, draw] (4) [below of = 2]{};
            \node[circle, draw] (5) [left of = 1]{};
            \node[circle, draw] (6) [right of = 2]{};

            \draw[-] (1) to (2);
            \draw[-] (1) to (3);
            \draw[-] (2) to (4);
            \draw[-] (1) to (5);
            \draw[-] (3) to (4);
            \draw[-] (3) to (5);
            \draw[-] (2) to (6);

            ;
        \end{tikzpicture}
        % \caption{An example of a vertex cover.}
    \end{figure}

    is evaluated through the following ratio $$\rho(G) := \dfrac{\abs E}{\abs V} = \dfrac{7}{6} = 1.1 \overline 6$$ The problem that we are going to introduce is the following.

    \begin{frameddefn}{Densest Subgraph problem}
        The \tbf{Densest Subgraph} (DS) problem is defined as follows: given an undirected graph $G = (V, E)$, determine the induced subgraph of $G$ that maximizes its density. In other words, the problem asks to find a set of vertices $S^*$ in $$S^* \in \argmax_{S \subseteq V}{\rho(G[S])}$$
    \end{frameddefn}

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[-,>=stealth,shorten >=1pt,auto,node distance=1.5cm, thick,main node/.style={scale=0.9,circle,draw,font=\sffamily\normalsize}]

            \node[circle, draw, fill=green] (1) []{1};
            \node[circle, draw, fill=green] (2) [below left of = 1]{2};
            \node[circle, draw, fill=green] (3) [below right of = 2]{3};
            \node[circle, draw, fill=green] (4) [below right of = 1]{4};
            \node[circle, draw] (5) [right of = 4]{5};

            \draw[-] (1) [green] to (2);
            \draw[-] (2) [green] to (3);
            \draw[-] (3) [green] to (4);
            \draw[-] (1) [green] to (4);
            \draw[-] (1) [green] to (3);
            \draw[-] (5) to (4);

            ;
        \end{tikzpicture}
        \caption{For instance, in this graph the set $S$ that maximizes $\rho(G[S])$ is given by $S = \{1, 2, 3, 4\}$, which yields a density of $\tfrac{5}{4} = 1.25$.}
    \end{figure}

    The densest subgraph is relevant because it actually \tit{maximizes} the average degree of its vertices. In fact, thanks to the handshaking lemma we have that $$\rho(G[S]) := \dfrac{\abs{E(G[S])}}{\abs S} = \dfrac{1}{2} \cdot \dfrac{2 \cdot \abs{E(G[S])}}{\abs s} = \dfrac{1}{2} \cdot \dfrac{\sum_{v \in S}{\deg (v)}}{\abs S} = \dfrac{1}{2} \avg_{v \in S}{\deg(v)}$$

    The LP that we will present which solves this problem is the following. However, to show that this LP actually yields a solution for DS is not trivial, and it will be proven through the following theorems.

    \begin{figure}[H]
        \centering
        \[\begin{array}{ccl}
            \qquad\qquad\quad
            & \max \; \displaystyle \sum_{e \in E} {x_e} \\\\
            & x_{ij} \le y_i & \forall i j \in E \\
            & x_{ij} \le y_j & \forall i j \in E \\
            & \sum\limits_{i \in V}{y_i} \le 1 \\
            & x_{ij} \ge 0 & \forall i j \in E \\
            & y_i \ge 0 & \forall i \in V \\
        \end{array}\]
        \caption{LP for DS.}
        \label{ds lp}
    \end{figure}

    \begin{framedlem}{}
        Given a graph $G = (V, E)$, for any $S \subseteq V$ there exists a feasible solution $\{x_e, y_i\}_{e \in E, i \in V}$ to the LP having value at least $\rho(G[S])$.
    \end{framedlem}

    \begin{proof}
        Given a set $S \subseteq V$, construct the following solution:

        \begin{itemize}
            \item $\forall ij \in E \quad x_{ij} := \soe{ll}{\dfrac{1}{\abs S} & i, j \in S \\ 0 & i \notin S \lor j \notin S}$
            \item $\forall i \in V \quad y_i := \soe{ll}{\dfrac{1}{\abs S} & i \in S \\ 0 & i \in V - S}$
        \end{itemize}

        We observe that

        \begin{itemize}
            \item $ij \in E(G[S]) \implies i, j \in S \implies \soe{l}{x_{ij} = \dfrac{1}{\abs S} \\ y_i = y_j = \dfrac{1}{\abs S}} \implies x_ij \le y_i, y_j$
            \item $ij \notin E(G[S]) \implies i \notin S \lor j \notin S \implies x_{ij} = 0$, but since $y_i, y_j \ge 0$ for any $i, j \in V$ by definition, it holds that $x_{ij} \le y_i, y_j$
        \end{itemize}

        Additionally, notice that $$\sum_{i \in V}{y_i} = \sum_{i \in S}{y_i} + \sum_{i \in V - S}{y_i} = \abs S \cdot \dfrac{1}{\abs S} + \abs {V - S} \cdot 0 = 1 \le 1$$

        This proves that this is a feasible solution to the LP. Lastly, the value of this solution is simply given by $$\sum_{e \in E}{x_e} = \sum_{e \in E(G[S])}{e} + \sum_{e \notin E(G[S])}{x_e} = \abs{E(G[S])} \cdot \dfrac{1}{\abs S} + \abs{E - E(G [S])} \cdot 0 = \dfrac{\abs{E(G[S])}}{\abs S} =: \rho(G[S])$$
    \end{proof}

    In particular, this lemma proves that if $S^*$ is an optimal solution for DS on a graph $G$, then there is a solution to this LP such that $\mathrm{LP^*} \ge \rho(G[S^*])$. The next theorem will prove that $\mathrm{LP^*} \le \rho(G[S^*])$ holds as well, which implies that our LP does in fact produce an optimal solution to DS, and that $\mathrm{IG_{DS}} = 1$.

    \begin{framedthm}{}
        Given a graph $G = (V, E)$, for any $\{x_e, y_i\}_{e \in E, i \in V}$ feasible solution to the LP that has value $v$, there exists a set $S \subseteq V$ such that $\rho(G[S]) \ge v$.
    \end{framedthm}

    \begin{proof}
        Consider a feasible solution $\{x_e', y_i'\}_{e \in E, i \in V}$ of the LP, having value $v$. We will construct another solution to the LP $\{x_e, y_i\}_{e \in E, i \in V}$ starting from the previous solution, as follows:

        \begin{itemize}
            \item $\forall i \in V \quad y_i := y_i'$
            \item $\forall ij \in E \quad x_{ij} := \min(y_i, y_j)$
        \end{itemize}

        It is easy to see that this is a feasible solution to the LP, in fact $$\forall ij \in E \quad x_{ij} := \min(y_i, y_j) \le y_i, y_j$$ and additionally, by feasibility of $\{x_e', y_i'\}_{e \in E, i \in V}$, we have that $$\sum_{i \in V}{y_i} = \sum_{i \in V}{y_i'} \le 1$$ Furthermore, note that $$v = \sum_{e \in E}{x_e'} \le \sum_{e \in E}{\min(y_i', y_j')} = \sum_{e \in E}{\min(y_i, y_j)} = \sum_{e \in E}{x_e}$$ meaning that the value of this solution is at least $v$, but it may be better --- this happens if $x_{ij} < y_i$ or $x_{ij} < y_j$ for some $ij \in E$, and if this is the case we say that there is some \tit{slack}.

        Next, we will construct a series of sets of vertices starting from this new LP solution we just defined. In particular, given a value $r \ge 0$, let $$S(r) := \{i \mid y_i \ge r\}$$ $$E(r) := \{e \mid x_e \ge r\}$$ Notice that $ij \in E(r) \iff i, j \in S(r)$, because $$ij \in E(r) \iff x_{ij} \ge r \iff \min(y_i, y_j) \ge r \iff y_i, y_j \ge r \iff i,j \in S(r)$$ which means that $S(r)$ and $E(r)$ are well-defined --- note that $E(G[S(r)]) = E(r)$.

        \claim{
            There exists a value $r^* \ge 0$ such that $\rho(G[S(r^*)]) \ge v$.
        }{
            Consider the solution $\{x_e, y_i\}_{e \in E, i \in V}$ to the LP that we constructed previously, and let $\pi$ be the permutation of the $y_i$'s such that $$0 \le y_{\pi (1)} \le y_{\pi (2)} \le \ldots \le y_{\pi(n - 1)} \le y_{\pi (n)}$$ which means that $\pi$ defines a sorting of the $y_i$'s. Now, consider the possible values of $r \ge 0$, and what happens inside $S(r)$:

            \begin{itemize}
                \item for $r = 0$, by feasibility of our solution we have that $$S(r) = S(0) := \{i \mid y_i \ge 0\} \implies \abs{S(r)} = n$$
                \item notice that for any value $0 \le r \le y_{\pi (1)} \le \ldots \le y_{\pi (n)}$, the value of $\abs{S(r)}$ will still be $n$, by its definition
                \item however, if $0 \le y_{\pi (1)} < r \le \ldots \le y_{\pi (n)}$, then the vertex $\pi(1)$ will be the only verterx not contained in $S(r)$, therefore $\abs{S(r)} = n - 1$
            \end{itemize}

            Repeating the same argument for all the possible values of $r$, we obtain the following graph for $\abs{S(r)}$.

            \begin{figure}[H]
                \centering
                \begin{tikzpicture}[scale=0.7]
                    \draw[-stealth] (-0.5,0) -- (2,0) node[below]{$y_{\pi(1)}$} -- (4,0) node[below]{$y_{\pi(2)}$} -- (7,0) node[below]{$\cdots$} -- (10,0) node[below]{$y_{\pi(n - 2)}$} -- (12,0) node[below]{$y_{\pi(n - 1)}$} -- (14,0) node[below]{$y_{\pi(n)}$} -- (15,0) node[right]{$r$};
                    \draw[-stealth] (0,-0.5) -- (0,0) node[below left]{0} -- (0,1) node[left]{1} -- (0,2) node[left]{2} -- (0,3.5) node[left]{$\cdots$} -- (0,5) node[left]{$n - 1$} -- (0,6) node[left]{$n$} -- (0,7.5) node[above]{$\abs{S(r)}$};
                    \draw[thick, orange] (0,6) -- (2,6) -- (2,5) -- (4,5) -- (4, 4) -- (5,4);
                    \draw[dashed, orange] (5,4) -- (6,4);
                    \draw[dashed, orange] (9,3) -- (8,3);
                    \draw[thick, orange] (14,0) -- (14,1) -- (12,1) -- (12,2) -- (10,2) -- (10,3) -- (9,3);
                \end{tikzpicture}
            \end{figure}

            From this graph, it is easy to see that computing the integral of $\abs{S(r)}$ is trivial, because
            \begin{equation*}
                \begin{split}
                    \int_{0}^{y_{\pi(n)}}{\abs{S(r)} \, d r} &= \int_{0}^{y_{\pi(1)}}{\abs{S(r)} \, dr} + \int_{y_{\pi(1)}}^{y_{\pi(2)}}{\abs{S(r)} \, dr} + \ldots + \int_{y_{\pi(n -1)}}^{y_{\pi(n)}}{\abs{S(r)} \, dr} \\
                                                             &= \int_{0}^{y_{\pi(1)}}{n \, dr} + \int_{y_{\pi(1)}}^{y_{\pi(2)}}{(n - 1) \, dr} + \ldots + \int_{y_{\pi(n - 1)}}^{y_{\pi(n)}}{1 \, dr} \\
                                                             &= n \cdot (y_{\pi(1)} - 0) + (n - 1) \cdot(y_{\pi(2)} - y_{\pi(1)}) + \ldots + 1 \cdot (y_{\pi(n)} - y_{\pi(n - 1)}) \\
                                                             &= y_{\pi(1)} \cdot\sbk{n - (n - 1)} + y_{\pi(2)}\cdot \sbk{(n - 1) - (n - 2)} + \ldots + y_{\pi(n)}\cdot (1 - 0) \\
                                                             &= y_{\pi(1)} \cdot 1 + y_{\pi(2)} \cdot 1 + \ldots + y_{\pi(n)} \cdot 1 \\
                                                             &= \sum_{i = 1}^n{y_{\pi(i)}} \\
                                                             &= \sum_{i \in V}{y_i} \\
                                                             &\le 1 \quad\quad (\mathrm{by \ feasibility \ of \ the \ solution})
                \end{split}
            \end{equation*}

            By the same argument, we can derive that $$\int_0^{y_{\pi(n)}}{\abs{E(r)} \, dr} = \sum_{e \in E}{x_e} \ge v$$ Note that the last inequality was proved before the claim.

            Lastly, we will prove the statement of the claim by way of contradiction. In particular, suppose that for all $r \ge 0$ it holds that $\rho(G[S(r)]) < v$. We can rewrite this inequality as follows $$\rho(G[S(r)]) := \dfrac{\abs{E(r)}}{\abs{S(r)}} < v \iff \abs{E(r)} < v \cdot \abs{S(r)}$$ This inequality can be used to upper bound the integral of $\abs{E(r)}$ \tit{point-by-point}, i.e. $$v \le \int_0^{y_{\pi(n - 1)}}{\abs{E(r)} \, dr} < \int_0^{y_{\pi(n - 1)}}{v \cdot \abs{S(r)} \, dr} = v \cdot \int_0^{y_{\pi(n - 1)}}{\abs{S(r)} \, dr} \le v \cdot 1 = v$$ meaning that $v < v$ $\lightning$.
        }

        Finally, since we proved that $G[S(r)]$ is well-defined, and this claim proves that there exists an $r^*$ such that $\rho(G[S(r^*)]) \ge v$, the statement holds.
    \end{proof}

    \subsection{Approximation through duality}

    As we discussed at the beginning of this section, the cost of solving an LP generally depends on the number of \tit{variables} and \tit{constraints} it has. In particular, the LP we presented for solving DS has a \tit{large} number of both, which means that while the cost remains \tit{polynomial}, the degree of the polynomial is too high, making this approach \tbf{impractical} for large graphs.

    Now, we are going to present a \tbf{greedy algorithm}, developed by \textcite{charikar} in 2000, and we will prove that this algorithm yields a $\tfrac{1}{2}$-approximation of DS.

    \begin{framedalgo}[label={charikar's algo}]{Charikar's algorithm}
        Given an undirected graph $G = (V, E)$, the algorithm returns a $\tfrac{1}{2}$-approximation solution of DS on $G$. \\
        \hrule

        \quad
        \begin{algorithmic}[1]
            \Function{Charikar}{$G$}
                \State $S_0 := V(G)$
                \For{$i \in [n - 1]$}
                    \State $\displaystyle v_i \in \argmin_{v \in S_{i - 1}}{\deg_{G[S_{i - 1}]}(v)}$
                    \State $S_i := S_{i - 1} - \{v_i\}$
                \EndFor
                \State \textbf{return} $\displaystyle S^* \in \argmax_{i \in [0, n - 1]}{\rho(G[S_i])}$
            \EndFunction
        \end{algorithmic}
    \end{framedalgo}

    \idea{
        The algorithm construct a series $S_0, \ldots, S_n$ of sets of vertices, such that $\abs{S_i} = n - i$, and at each iteration the $i$-th vertex that gets removed from $G[S_{i - 1}]$ is the one having minimum degree. At the end, the algorithm returns the set $S^*$ that maximizes the density of $G[S^*]$.
    }

    We observe that this algorithm does not return an optimal solution in general; in particular, problems can arise if there are vertices of equal degree. For instance, consider the following disconnected graph

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[-,>=stealth,shorten >=1pt,auto,node distance=2cm, thick,main node/.style={scale=0.9,circle,draw,font=\sffamily\normalsize}]

            \node[circle, draw] (1) []{$a$};
            \node[circle, draw] (2) [right of = 1]{$b$};
            \node[circle, draw] (3) [right of = 2]{$c$};
            \node[circle, draw] (4) [right of = 3]{$d$};
            \node[circle, draw] (5) [right of = 4]{$e$};

            % \path[every node/.style={font=\sffamily\small}]

            \draw[-] (1) to (2);
            \draw[-] (2) to (3);
            \draw[-] (4) to (5);

            ;
        \end{tikzpicture}
    \end{figure}

    In this graph, the densest subgraph is induced by the set $S^* = \{a, b, c\}$, because $$\rho(G[S^*]) = \dfrac{2}{3}$$ However, since $\deg(a) = \deg(c) = \deg(d) = \deg(e) = 1$, the algorithm may choose $v_1 := a$, meaning that $G[S_1]$ would be 

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[-,>=stealth,shorten >=1pt,auto,node distance=2cm, thick,main node/.style={scale=0.9,circle,draw,font=\sffamily\normalsize}]

            \node[circle, draw] (2) []{$b$};
            \node[circle, draw] (3) [right of = 2]{$c$};
            \node[circle, draw] (4) [right of = 3]{$d$};
            \node[circle, draw] (5) [right of = 4]{$e$};

            % \path[every node/.style={font=\sffamily\small}]

            \draw[-] (2) to (3);
            \draw[-] (4) to (5);

            ;
        \end{tikzpicture}
    \end{figure}

    and it is easy to see that $G[S_0] = G$, and any other subgraph of this graph, have lower density than $G[S^*]$.

    To prove that this algorithm yields a $\tfrac{1}{2}$-approximation, we are going to introduce some definitions first.

    \begin{frameddefn}{Orientation}
        Given an undirected graph $G = (V, E)$, an \tbf{orientation} of $G$ is a function $\func{\phi}{E}{V}$ that assigns to each edge $\{u, v\} \in E$ either $u$ or $v$.
    \end{frameddefn}

    Note that, by definition for any $e \in E$ it holds that $\phi(e) \in e$. For instance, given the following graph

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[-,>=stealth,shorten >=1pt,auto,node distance=2cm, thick,main node/.style={scale=0.9,circle,draw,font=\sffamily\normalsize}]

            \node[circle, draw]  (1) []{$a$};
            \node[circle, draw]  (2) [above right of = 1]{$c$};
            \node[circle, draw]  (3) [below right of = 2]{$b$};
            \node[circle, draw]  (4) [above right of = 3]{$d$};

            % \path[every node/.style={font=\sffamily\small}]

            \draw[-] (1) to (2);
            \draw[-] (2) to (3);
            \draw[-] (2) to (4);
            \draw[-] (3) to (4);

            ;
        \end{tikzpicture}
        % \caption{Given the set of red vertices $S$, the green edges represent $\mathrm{cut}(S)$.}
    \end{figure}

    and the following function $\phi$

    \begin{center}
        \begin{tabular}{c|c c} 
             \hline
             $e$ & $\phi(e)$ \\
             \hline\hline
             $ac$ & $c$ \\ 
             \hline
             $bc$ & $c$ \\
             \hline
             $bd$ & $b$ \\
             \hline
             $cd$ & $d$ \\
             \hline
        \end{tabular}
    \end{center}

    we would get the following orientation on $G$

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[-,>=stealth,shorten >=1pt,auto,node distance=2cm, thick,main node/.style={scale=0.9,circle,draw,font=\sffamily\normalsize}]

            \node[circle, draw]  (1) []{$a$};
            \node[circle, draw]  (2) [above right of = 1]{$c$};
            \node[circle, draw]  (3) [below right of = 2]{$b$};
            \node[circle, draw]  (4) [above right of = 3]{$d$};

            % \path[every node/.style={font=\sffamily\small}]

            \draw[->] (1) to (2);
            \draw[->] (3) to (2);
            \draw[->] (2) to (4);
            \draw[->] (4) to (3);

            ;
        \end{tikzpicture}
        \caption{The previous graph, oriented through $\phi$.}
        \label{oriented graph}
    \end{figure}

    Given an orientation $\phi$ of $G$, and a vertex $v \in V(G)$ we will indicate with $\deg_\phi(v)$ the in-degree of $v$ w.r.t. $\phi$, i.e. $$\deg_\phi(v) := \abs{\{e \in E \mid \phi(e) = v\}}$$ For example, in the previous graph we have that $\deg_\phi(c) = 2$, $\deg_\phi(a) = 0$ and $\deg_\phi(b) = \deg_\phi(d) = 1$. By the same argument that proves the handshaking lemma, we observe that $$\sum_{v \in V(G)}{\deg_\phi(v)} = \abs E$$ Lastly, the maximum in-degree w.r.t. $\phi$ will be denoted as $$\Delta_\phi := \max_{v \in V} {\deg_\phi(v)}$$ We are now ready to prove the approximation ratio of Charikar's algorithm.

    \begin{framedthm}[label={charikar approx}]{\textsc{Charikar}'s approximation ratio}
        Given a graph $G$, and an optimal solution $\rho(G[S^*])$ for DS on $G$, let $M := \textsc{Charikar}(G)$; then, it holds that $\tfrac{1}{2} \rho(G[S^*]) \le M$.
    \end{framedthm}

    \begin{proof}
        We are going to prove this approximation ratio through two claims.

        \claim{
            Given an undirected graph $G$, and an optimal solution $\rho(G[S^*])$ for DS on $G$, for any orientation $\phi$ of $G$ it holds that $\rho(G[S^*]) \le \Delta_\phi$.
        }{
            By definition of $\phi$, each $uv \in E$ is going to be oriented towards either $u$ or $v$, therefore if $uv \in E(G[S^*])$ then $uv$ is going to be oriented towards a node of $S$. Hence, we have that \centeredeq{0.99}{$\displaystyle \abs{E(G[S^*])} \le \sum_{v \in V(G[S^*])}{\deg_\phi(v)} \le \sum_{v \in V(G[S^*])}{\Delta_\phi} = \abs {V[G(S^*)]} \cdot \Delta_\phi \iff \rho(G[S^*]) := \dfrac{\abs{E(G[S^*]})}{\abs{V(G[S^*])}} \le \Delta_\phi$}
        }

        Consider Charikar's algorithm; before proving the other claim, we need to introduce an orientation $\phi_\mathrm{GR}$ --- where GR stands for \tit{greedy} --- of $G$ defined as follows: if $v_i$ is the $i$-th vertex --- removed from $G[S_{i - 1}]$ --- picked by the algorithm, orient any edge $\{u, v_i\} \in E(G)$ towards $v_i$, for any $u \in V(G)$. For instance, Charikar's algorithm applied on the graph shown in \cref{oriented graph} would set $v_1 := a$, therefore $\phi(ca) = a$, and so on. This implies that $$\forall i \in [n] \quad \deg_{G[S_{i - 1}]}(v_i) = \deg_{\phi_\mathrm{GR}}(v_i)$$

        \claim{
            Given an undirected graph $G$, and $M := \textsc{Charikar}(G)$, it holds that $\Delta_{\phi_\mathrm{GR}} \le 2M$.
        }{
            Observe that for any $i \in [n]$ it holds that $$v_i \in \argmin_{v \in S_{i - 1}}{\deg_{G[S_{i - 1}]}(v)} \iff \deg_{G[S_{i - 1}]}(v_i) = \min_{v \in S_{i - 1}}{\deg_{G[S_{i - 1}]}(v)}$$ therefore, we have that 
            \begin{equation*}
                \begin{split}
                    \deg_{\phi_\mathrm{GR}}(v_i) &= \deg_{G[S_{i - 1}]}(v_i) \\
                                                 &= \min_{v \in S_{i - 1}}{\deg_{G[S_{i - 1}]}(v)} \\
                                                 &\le \avg_{v \in S_{i - 1}}{\deg_{G[S_{i - 1}]}(v)} \\
                                                 &= \dfrac{\displaystyle \sum_{v \in S_{i - 1}}{\deg_{G[S_{i - 1}]}(v)}}{\abs{S_{i - 1}}} \\
                                                 &= \dfrac{2 \abs{E(G[S_{i - 1}])}}{\abs{V(G[S_{i - 1}])}} \\
                                                 &=: 2\rho(G[S_{i - 1}])
                \end{split}
            \end{equation*}
            Finally, we get that
            \begin{equation*}
                \begin{split}
                    \Delta_{\phi_\mathrm{GR}} &:= \max_{v \in V(G)}{\deg_{\phi_\mathrm{GR}}(v)} \\
                                              &= \max_{i \in [n]}{\deg_{\phi_\mathrm{GR}}(v_i)} \\
                                              &\le \max_{i \in [n]}{2 \rho (G[S_{i - 1}])} \quad \quad (\mathrm{for \ the \ previous \ inequality}) \\
                                              &= 2 \max_{i \in [n]}{\rho(G[S_{i - 1}])} \\
                                              &\le 2 \max_{i \in [0, n - 1]}{\rho(G[S_i])} \\
                                              &=: 2M
                \end{split}
            \end{equation*}
        }

        Lastly, putting the two claims together, we get that for any optimal solution $\rho(G[S^*])$ of a given undirected graph $G$, if $M := \textsc{Charikar}(G)$ then $$\rho(G[S^*]) \le \Delta_{\phi_\mathrm{GR}} \le 2M \iff \tfrac{1}{2} \rho(G[S^*]) \le M$$
    \end{proof}

    In this proof, we defined $\phi_\mathrm{GR}$, which helped us determine the approximation ratio of the algorithm. But how did Charikar come up with such an orientation of the graph? To answer this question, we need to introduce the concept of the \tbf{duality} of linear programs.

    Consider the following \tit{maximization} generic linear program:

    \begin{figure}[H]
        \centering
        \[\begin{array}{cc}
            % \qquad\qquad\quad
            & \max \; c_1x_1 + \ldots + c_n x_n \\\\
            & a_{11}x_1 + \ldots + a_{1n}x_n \le b_1 \\
            & \vdots \\
            & a_{m1}x_1 + \ldots + a_{mn}x_n \le b_m \\
            & x_1, \ldots, x_n \ge 0
        \end{array}\]
        \caption{A \tit{primal} LP.}
    \end{figure}

    As a convention, we will call the \tbf{primal} LP the one that \tit{maximizes} its objective function. Hence, given a primal LP, its \tbf{dual} linear program is defined as follows:

    \begin{figure}[H]
        \centering
        \[\begin{array}{cc}
            % \qquad\qquad\quad
            & \min \; b_1y_1 + \ldots + b_m y_m \\\\
            & a_{11}y_1 + \ldots + a_{m1}y_m \ge c_1 \\
            & \vdots \\
            & a_{1n}y_1 + \ldots + a_{mn}y_m \ge c_n \\
            & y_1, \ldots, y_m \ge 0
        \end{array}\]
        \caption{The \tit{dual} of a primal LP.}
    \end{figure}

    In other words, given a matrix $A \in \R^{m \times n}$, two vectors $c, x \in \R^n$, and two vectors $b, y \in \R^m$, these two programs can be rewritten as follows:
     \begin{figure}[H]
        \centering

        \begin{tabular}{ccccc}
            $\begin{array}{cc}
                % \qquad\qquad\quad
                & \mathrm{\underline{Primal \ LP}} \\\\
                & \max \; c^Tx \\\\
                & Ax \le b \\
                & x \ge 0
            \end{array}
            $

            &\qquad& $\iff$ &\qquad&

            $
            \begin{array}{cc}
                % \qquad\qquad\quad
                & \mathrm{\underline{Dual \ LP}} \\\\
                & \min \; b^Ty \\\\
                & A^Ty \ge c \\
                & y \ge 0
            \end{array}
            $
        \end{tabular}
        % \caption{On the left: a \tit{primal} LP. On the right: its \tit{dual} LP.}
    \end{figure}

    Observe that, by definition, the dual of the dual of a primal LP is the primal LP itself. Moreover, we can relate the solutions of primal LPs and their duals through the following two theorems.

    \begin{framedthm}{Weak duality theorem}
        Given a feasible solution $x$ for a primal LP, and a feasible solution $y$ for its dual LP, such that both $x$ and $y$ exist and are finite, it holds that $$c^T x \le b^T y$$
    \end{framedthm}

    \begin{proof}
        Given any two matrices $N \in \R^{m \times n}$, $M \in \R^{n \times l}$ it holds that $$(NM)^T = M^TN^T$$ therefore, we have that
        \begin{equation*}
            \begin{split}
                c^Tx &= x^Tc \\
                     &\le x^TA^Ty \quad \quad (\mathrm{by \ feasibility \ of} \ y) \\
                     &= (Ax)^Ty \quad \quad (\mathrm{by \ the \ previous \ observation}) \\
                     &\le b^Ty \quad \quad \quad (\mathrm{by \ feasibility \ of} \ x)
            \end{split}
        \end{equation*}
    \end{proof}

    \begin{framedthm}{Strong duality theorem}
        Given an optimal solution $x$ for a primal LP, and an optimal solution $y$ for its dual LP, such that both $x$ and $y$ exist and are finite, it holds that $$c^Tx = b^Ty$$
    \end{framedthm}

    Going back to our problem, consider again the LP for DS presented in \cref{ds lp}. We observe that its dual LP is the following linear program:

    \begin{figure}[H]
        \centering
        \[\begin{array}{ccl}
            \qquad\qquad\quad
            & \min \; \delta \\\\
            & \gamma_{ij} + \gamma_{ji} \ge 1 & \forall i j \in E \\
            & \delta - \sum\limits_{\substack{j \in V : \\ ij \in E}}{\gamma_{ij}} \ge 0 & \forall i \in V \\
            & \delta \ge 0 \\
            & \gamma_{ij} \ge 0 & \forall i j \in E \\
            & \gamma_{ji} \ge 0 & \forall i j \in E \\
        \end{array}\]
        \caption{The dual of the LP for DS.}
    \end{figure}
    
    Notice that this is the LP that corresponds to the \tbf{Minimum Max-Degree Orientation} problem, which asks to find the edge orientation with the lowest possible maximum degree. In fact, observe that $$\forall i \in V \quad \delta - \sum_{\substack{j \in V : \\ ij \in E}}{\gamma_{ij}} \ge 0 \iff \delta \ge \deg(i)$$

    TODO \todo{missing last part}

    Even though this approach yields a $\tfrac{1}{2}$-approximation algorithm that runs in only $n$ iterations, it may still be too slow for very large graphs. Can we improve the algorithm further? Consider how \textsc{Charikar} algorithm works: at each iteration, it removes the vertex with the minimum degree from the current graph. What if, instead of removing just one such vertex per iteration, we remove all vertices of minimum degree \tit{at once}?

    \begin{framedalgo}{Charikar's algorithm (improved version)}
        Given an undirected graph $G = (V, E)$, the algorithm returns a $\tfrac{1}{2}$-approximation solution of DS on $G$. \\
        \hrule

        \quad
        \begin{algorithmic}[1]
            \Function{CharikarImproved}{$G$}
                \State $S_0 := V(G)$
                \State $i := 0$
                \While{$S_i \neq \varnothing$}
                    \State $\displaystyle A_i := \argmin_{u \in S_i}{\deg_{G[S_i]}(u)}$
                    \State $S_{i + 1} := S_i - A_i$
                \EndWhile
                \State \textbf{return} $\displaystyle S^* \in \argmax_{i \in \N}{\rho(G[S_i])}$
            \EndFunction
        \end{algorithmic}
    \end{framedalgo}

    This algorithm is able to reduce significantly the number of iterations, by removing the vertices of minimum degree from $S_i$ all at once. However, in the worst case the number of iterations is still $n$. For instance, consider the following graph 

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[-,>=stealth,shorten >=1pt,auto,node distance=1.5cm, thick,main node/.style={scale=0.9,circle,draw,font=\sffamily\normalsize}]

            \node[circle, draw] (1) []{1};
            \node[circle, draw] (2) [right of = 1]{2};
            \node[circle, draw] (3) [right of = 2]{3};
            \node[circle, draw] (4) [right of = 3]{4};
            \node[circle, draw] (5) [above right of = 4]{5};
            \node[circle, draw] (6) [below right of = 4]{6};

            % \path[every node/.style={font=\sffamily\small}]

            \draw[-] (1) to (2);
            \draw[-] (2) to (3);
            \draw[-] (3) to (4);
            \draw[-] (4) to (5);
            \draw[-] (4) to (6);
            \draw[-] (5) to (6);

            ;
        \end{tikzpicture}
        % \caption{Given the set of red vertices $S$, the green edges represent $\mathrm{cut}(S)$.}
    \end{figure}

    In this graph, our improved version of Charikar would still need $n$ iterations to remove all the vertices --- in the figure, the node labeled with $i$ is the only node present in $A_i$ at the $i$-th iteration of the algorithm. Can we do better?

    Consider the proof of \cref{charikar approx}; the second claim of the theorem uses the fact that the vertex $v_i$ has minimum degree in $G[S_{i - 1}]$. However, note that the only reason why we need this fact is to bound $$\deg_{G[S_{i - 1}]}(v_i) \le \avg_{v \in S_{i - 1}}{\deg_{G[S_{i - 1}]}(v_i)}$$ Hence, we may try to use the \tit{average degree} instead of the \tit{minimum degree}, in the definition of $A_i$.

    Consider a path $P_n$; observe that $\textsc{CharikarImproved}(P_n)$ would require $\ceil{\tfrac{n}{2}}$ iteration to remove all the vertices from the initial graph, because it would remove at most (for the case when $n$ is odd) 2 vertices per iteration --- i.e. the endpoints. What happens if we replace the definition of $A_i$ as follows? $$A_i := \cbk{v \in S_i \mid \deg_{G[S_i]}(v) \le \avg_{u \in S_i}{\deg_{G[S_i]}(u)}}$$ In the case of $P_n$, \textsc{CharikarImproved} would still fail at processing the graph all at once, because the average degree is \tit{less than} 2. Nonetheless, this problem suggest the following approach.

    \begin{framedalgo}{Charikar's algorithm ($\varepsilon$ version)}
        Given an undirected graph $G = (V, E)$, the algorithm returns a $\tfrac{1}{2(1 + \varepsilon)}$-approximation solution of DS on $G$. \\
        \hrule

        \quad
        \begin{algorithmic}[1]
            \Function{$\textsc{Charikar}_\varepsilon$}{$G$}
                \State $S_0 := V(G)$
                \State $i := 0$
                \While{$S_i \neq \varnothing$}
                    \State $\displaystyle A_i := \cbk{v \in S_i \mid \deg_{G[S_i]}(v) \le (1 + \varepsilon)\avg_{u \in S_i}{\deg_{G[S_i]}(u)}}$
                    \State $S_{i + 1} := S_i - A_i$
                \EndWhile
                \State \textbf{return} $\displaystyle S^* \in \argmax_{i \in \N}{\rho(G[S_i])}$
            \EndFunction
        \end{algorithmic}
    \end{framedalgo}

    \begin{framedthm}{$\textsc{Charikar}_\varepsilon$'s approximation ratio}
        Given a graph $G$, and $S := \textsc{Charikar}_\varepsilon$(G), let $S^*$ be an optimal solution to DS on $G$. Then, for all $\varepsilon > 0$ it holds that $\abs S \ge \tfrac{1}{2(1 + \varepsilon)} \abs{S^*}$. Moreover, the algorithm runs in at most $O(\log_{1 + \varepsilon}(n))$ iterations.
    \end{framedthm}

    \begin{proof}
        Consider the optimal solution $S^*$; note that $S^* \neq \varnothing$ because $\rho(G[\varnothing])$ is not defined, and if $\abs{S^*} = 1$, then $\abs{E(G[S^*])} = 0 \implies \rho(G[S^*]) = 0$ meaning that \todo{non ho capito}. Hence, we may assume that $\abs{S^*} \ge 2$.
        
        \claim{
            $\forall v \in S^* \quad \deg_{G[S^*]}(v) \ge \rho(G[S^*])$
        }{
            Fix a vertex $v \in S^*$. By optimality of $S$, we know that $\rho(G[S^*]) \ge \rho(G[S^* - \{v\}])$. Moreover, note that $$\rho(G[S^* - \{v\}]) = \dfrac{\abs{E(G[S^* - \{v\}])}}{\abs{S^* - \{v\}}} = \dfrac{\abs{E(G[S^*])} - \deg_{G[S^*]}(v)}{\abs{S^*} - 1}$$ Therefore, we have that $$\rho(G[S^*]) \ge \rho(G[S^* - \{v\}]) \iff \dfrac{\abs{E(G[S^*])}}{\abs{S^*}} \ge \dfrac{\abs{E(G[S^*])} - \deg_{G[S^*]}(v)}{\abs{S^*} - 1}$$ and the claim follows by solving the inequality.
            
            TODO \todo{da finire}
        }
    \end{proof}

    TODO \todo{buco}

    \section{The Unique Games Conjecture}

    Before discussing the conjecture itself, consider the following definition.

    \begin{frameddefn}{Unique label cover}
        Let $G$ be a bipartite graph, bipartitioned through $(A, B)$; given a value $k \in \N$, for each edge $e \in E(G)$ let $\func{\pi_e}{[k]}{[k]}$ be a permutation. A \tbf{unique label cover} (ULC) of $G$ is an assignment $\func{\phi}{A \cup B}{[k]}$ defining the set $S_\phi$ of satisfied edges: $$S_\phi := \{ab \in E(G) \mid a \in A, b \in B, \pi_{ab}(\phi(a)) = \phi(b)\}$$
    \end{frameddefn}

    In other words, an edge $ab \in E(G)$ of $G$ is sait to be \tit{satisfied} by $\phi$ if the color $\phi(a)$ gets permuted into the color $\phi(b)$ by the permutation $\pi_{ab}$.

    \centeredimage[In this figure, (a) and (b) are two instances of the UCL problem with 2 colors; (b) is a solution to (a) that satisfies all the edges, while (d) is a solution to (c) with an unsatisfied edge.]{0.99}{../assets/ucl.png}

    Note that UCL instances are \tit{strongly constrainted}, i.e. the color of a vertex uniquely defines the colors of its neighbors, and therefore its entire connected component. Thus, if the input instance admits a valid assignment, such assignment can be found \tit{efficiently} by iterating over all the possible colors of a single node. In particular, this implies that the problem of deciding if a given instance admits a satisfying assignment can be solved in polynomial time.

    The \tbf{value} of a UCL instance is the ratio of the edges that are satisfiable by any assignment. Hence, for satisfiable instances the ratio is 1, and we can find a satisfiying assignment as described. In contrast, determining the value of an unsatisfiable game --- even approximately --- appears to be very hard. This difficulty was formalized by \textcite{khot} in terms of \NPclass-hardness, as follows.

    \begin{framedconj}{Unique Games Conjecture (UGC)}
        It is conjectured that for each $\varepsilon > 0$ there is a value $k_\varepsilon$ for which it is $\NPclass-Hard$ to determine if for a UCL instance with $k_\varepsilon$ labels one of the following holds:

        \begin{itemize}
            \item at most an $\varepsilon$-fraction of the edges are satisfied
            \item at least a $(1 - \varepsilon)$-fraction of the edges are satisfied
        \end{itemize}
    \end{framedconj}

    Furthermore, Khot's conjecture has been shown to be linked with \href{https://en.wikipedia.org/wiki/Constraint_satisfaction_problem}{Constraint Satisfaction Problems}, a particular type of optimization problems defined as follows.

    \begin{frameddefn}{Constraint Satisfaction Problem}
        Let $\mathcal P$ be a set of $k$-ary predicates defined on $[q]$, for $q, k \in \N$. An instance of a \tbf{Constraint Satisfaction Problem} (CSP) is a set of variables $X = \{x_1, \ldots, x_n\}$ and a set of constraints $C_1, \ldots, C_m$ such that $C_j = \abk{I_j, P_j}$ where $I_j \subseteq [n]$ and $P_j \in \mathcal P$.

        A constraint $C_j = \abk{I_j, P_j}$ is said to be \tit{satisfied} by an assignment $\func{\alpha}{I_j}{\{0, 1\}}$ if $P_j \smat{x_{i_1} & \ldots & x_{i_k} \\ \alpha(x_{i_1}) & \ldots & \alpha(x_{i_k})}$ evaluates to true.
    \end{frameddefn}

    TODO \todo{buco}
    
    Given a problem that can be described through a CSP $\mathcal C$, let $\alpha(\mathcal C)$ be the best possible approximation ratio for $\mathcal C$, i.e. the minimum ratio such that the problem does not become \NPclass-Hard to approximate. The following theorem, proved by \textcite{raghavendra}, shows that the relationship between approximation algorithms, SDPs, and the UGC is as strong as it can be.

    \begin{framedthm}{Raghavendra's theorem}
        For every CSP $\mathcal C$ with values in $[q]$ and arity $k$, the following hold.

        \begin{itemize}
            \item There is an SDP with IG of $\alpha(\mathcal C)$ and a rounding algorithm that is an $\alpha(\mathcal C)$-approximation of $\mathcal C$.
            \item If the UGC is true, it is \NPclass-Hard to approximate $\mathcal C$ with a ratio $\alpha(\mathcal C) - \varepsilon$ for any $\varepsilon > 0$.
        \end{itemize}
    \end{framedthm}

    In particular, if the UGC holds true, Raghavendra's theorem implies that the SDP integrality gap for any problem that can be described through a CSP is the best possible approximation ratio that can be achieved. Although many researchers believe in the validity of the conjecture, it remains unproven.

    As a direct corollary of Raghavendra's theorem, we get that the approximation ratios that we previously found for MC and VC are the best possible ones, if the UGC is true.

    \begin{framedcor}{}
        If the UGC is true, then $\mathrm{IG}_{\mathrm{MC}}^{\mathrm{SDP}} = \alpha(\mathrm{MC})$ and $\mathrm{IG}_\mathrm{VC} = \alpha(\mathrm{VC})$.
    \end{framedcor}

    \section{Exercises}
    
    \begin{framedprob}{}
        Show that, for each $\varepsilon > 0$, there exists a graph $G$ such that \cref{charikar's algo} returns a subgraph of $G$ having density not larger than $\tfrac{1}{2} + \varepsilon$ times the optimal --- i.e. the approximation bound of $\tfrac{1}{2}$ is optimal.
    \end{framedprob}

    \solution{
        TODO \todo{da fare}

        \begin{figure}[H]
            \centering
            \begin{tikzpicture}[-,>=stealth,shorten >=1pt,auto,node distance=1.5cm, thick,main node/.style={scale=0.9,circle,draw,font=\sffamily\normalsize}]

                \node[circle, draw] (1) []{};
                \node[circle, draw] (2) [below of = 1, yshift = 0.5cm]{};
                \node[circle, draw] (3) [below of = 2, yshift = 0.5cm]{};
                \node[circle, draw] (4) [below of = 3, yshift = 0.5cm]{};
                \node[circle, draw] (5) [left of = 2]{};
                \node[circle, draw] (6) [left of = 3]{};

                \node[circle, draw] (7) [right of = 1, yshift = -0.625cm]{};
                \node[circle, draw] (8) [right of = 7]{};
                \node[circle, draw] (9) [right of = 4, yshift = 0.625cm]{};
                \node[circle, draw] (10) [right of = 9]{};

                \node[circle, draw] (11) [right of = 8]{};
                \node[circle, draw] (12) [right of = 11]{};
                \node[circle, draw] (13) [right of = 10]{};
                \node[circle, draw] (14) [right of = 13]{};

                % \path[every node/.style={font=\sffamily\small}]

                \draw[-] (1) to (5);
                \draw[-] (1) to (6);
                \draw[-] (2) to (5);
                \draw[-] (2) to (6);
                \draw[-] (3) to (5);
                \draw[-] (3) to (6);
                \draw[-] (4) to (5);
                \draw[-] (4) to (6);

                \draw[-] (7) to (8);
                \draw[-] (7) to (9);
                \draw[-] (7) to (10);
                \draw[-] (8) to (9);
                \draw[-] (8) to (10);
                \draw[-] (9) to (10);

                \draw[-] (11) to (12);
                \draw[-] (11) to (13);
                \draw[-] (11) to (14);
                \draw[-] (12) to (13);
                \draw[-] (12) to (14);
                \draw[-] (13) to (14);

                ;
            \end{tikzpicture}
            \caption{An example of the graph $G$ with $t = 2$, consisting of a $K_{2,4}$ and two $K_4$'s.}
        \end{figure}
    }

    \chapter{Metric geometry}

    TODO \todo{missing introduction}

    First, consider the following definition

    \begin{frameddefn}{Set spartity}
        Given a graph $G = (V, E)$, and a set $\varnothing \subset S \subseteq V$, the \tbf{sparsity} of $S$ is defined as follows: $$\psi (G) := \dfrac{\abs{\mathrm{cut}(S)}}{\abs{S \times \overline S}} = \dfrac{\abs{\mathrm{cut}(S)}}{\abs S \cdot \abs{\overline S}}$$
    \end{frameddefn}

    As for the case of \tit{graph density}, we require $S \neq \varnothing$, but we will omit this detail in the following sections. Moreover, note that by definition of \tit{cut}, we have that for any $S \subseteq V(G)$ it holds that $$0 \le \abs{\mathrm{cut}(S)} \le \abs {S \times \overline S} = \abs S \cdot \abs{\overline S}$$ since an edge $e$ is in $\mathrm{cut}(S)$ if and only if $\abs {S \cap e} = 1$, hence the cartesian product $S \times \overline S$ represents the edges that have exactly one endpoint in $S$. Therefore, we have that $0 \le \psi(S) \le 1$, and in particular

    \begin{itemize}
        \item $\psi(S) = 0 \implies \abs{\mathrm{cut}(S)} = 0$ which happens if $S$ is \tit{disconnected} from $\overline S$
        \item $\psi(S) = 1 \implies \abs{\mathrm{cut}(S)} = \abs{S \times \overline S}$ which happens if $S$ is \curlyquotes{\tit{fully connected}} to $\overline S$, i.e. every edge of $S$ is connected to every edge of $\overline S$.
    \end{itemize}

    Given this metric, we are interested in finding the subset of vertices of a given graph that \tit{minimizes} its sparsity.

    \begin{frameddefn}{Sparsest Cut problem}
        The \tbf{Sparsest Cut} (SC) problem is defined as follows: given a graph $G = (V, E)$, determine the subset of vertices that minimizes its sparsity. In other words, the problems asks to finde a set $S^*$ in $$S^* \in \argmin_{S \subseteq V}{\psi(S)}$$
    \end{frameddefn}

    In order to reason about SC, we are going to introduce the following type of graph, invented by \textcite{erdos} in 1959.

    \begin{frameddefn}{ErdÅs-RÃ©nyi random graph}
        Given two values $n \in \N$ and $\mu \in [0, 1]$, the \href{https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model}{ErdÅs-RÃ©nyi random graph} $G(n, \mu)$ is a graph that has a fixed vertex set $V(G) = [n]$, and a \tit{probabilistic} edge set $E(G)$ such that $$\forall e \in [V(G)]^2 \quad \Pr[e \in E(G)] = \mu$$
    \end{frameddefn}

    By definition, this model represents an \tit{evenly sparse graph}, because for each $S \subseteq V(G)$ it holds that $$ \mathbb E[\psi(S)] = \dfrac{\mathbb E\sbk{\abs{\mathrm{cut}(S)}}}{\abs S \cdot \abs{\overline S}} = \dfrac{\sum_{ij \in S \times \overline S}{\Pr \sbk{ij \in \mathrm{cut}(S)}}}{\abs S \cdot  \abs{\overline S}}= \dfrac{\mu \cdot \abs S \cdot \abs{\overline S}}{\abs S \cdot \abs{\overline S}} = \mu$$

    The Maximum Cut problem --- which we discussed in \cref{maxcut} --- can be reduced to SC, implying that the latter is \NPclass-Hard as well. Nevertheless, the problem can be approximated through the \tbf{Leighton-Rao algorithm} \cite{leightonrao}, which relies on LP relaxation and a series of reduction steps. In particular, its integrality gap is bounded using \tit{geometric arguments}, therefore in the following sections we will focus on \tbf{metric geometry} in order to establish this result.

    \section{Metrics}

    \subsection{Cut metrics}

    A \tbf{metric} (or \tit{distance function}) provides a way to quantify how far apart objects are within a given space, formalizing the concept of \tit{distance}. In particular, in geometry we have the following definition.

    \begin{frameddefn}{Metric}
        Given a set $S$, a \tbf{metric} on $S$ is a function $\func{d}{S \times S}{\R}$ that satisfies

        \begin{enumerate}
            \item \tit{non-negativity}: $\forall x, y \in S \quad d(x,y) \ge 0$
            \item \tit{simmetry}: $\forall x, y \in S \quad d(x, y) = d(y, x)$
            \item \tit{self-distance}: $\forall x \in S \quad d(x, x) = 0$
            \item \tit{triangle inequality}: $\forall x, y, z \in S \quad d(x,y) \le d(x,z) + d(z, y)$
        \end{enumerate}
    \end{frameddefn}

    The typical example of metric is the \href{https://en.wikipedia.org/wiki/Euclidean_distance}{Euclidean distance}, which defines $d$ as follows $$d(x, y) := \norm{x - y}$$ In particular, for our discussion we are interested in \tit{cut metrics}, defined as follows.

    \begin{frameddefn}{Elementary cut metric}
        Given a graph $G = (V, E)$, and a set $T \subseteq V$, the \tbf{elementary cut metric of $T$} is a function $\func{d_T}{V \times V}{\R}$ such that $$d_T(x, y) := \soe{ll}{1 & \abs{T \cap \{x, y\}} = 1 \\ 0 & \mathrm{otherwise}}$$
    \end{frameddefn}

    In other words, for a given set of vertices $T$, the elementary cut metric $d_T(x, y)$ is equal to 1 if and only if the edge $xy$ is in $\mathrm{cut}(T)$.

    \begin{framedprop}{}
        Any elementary cut metric is a metric.
    \end{framedprop}

    \begin{proof}
        Given a graph $G$, and a susbset $T \subseteq V(G)$, consider the elementary cut metric $d_T$; we observe that, by definition

        \begin{itemize}
            \item \tit{non-negativity} is satisfied, because $$\forall x, y \in V(G) \quad d_T(x, y) = 0 \lor d_T(x, y) = 1 \implies d_T(x, y) \ge 0$$
            \item \tit{simmetry} is satisfied, because $$\forall x, y \in V(G) \quad \abs{T \cap \{x, y\}} = \abs{T \cap \{y, x\}} \implies d_T(x, y) = d_T(y, x)$$
            \item \tit{self-distance} is satisfied, because $$\forall x \in V(G) \quad \nexists \{x, x\} \in E(G) \implies d_T(x, x) =0$$
        \end{itemize}
        
        hence, we just need to prove that the \tit{triangle inequality} is also satisfied by $d_T$. Therefore, fix three vertices $x, y, z \in V(G)$; we have three cases:

        \begin{itemize}
            \item if $x, y, z \in T$ then $$0 = d_T(x, y) \le d_T(x, z) + d_T(z, y) = 0 + 0 = 0$$
            \item if $x, y, z \notin T$ then $$0 = d_T(x, y) \le d_T(x, z) + d_T(z, y) = 0 + 0 = 0$$
            \item if $\exists A \in \{T, \overline T\}$ such that \tit{exactly one} of $x, y, z$ lies in $A$, we have three sub-cases:
                \begin{itemize}
                    \item if $x \in A$ then $$1 = d_T(x, y) \le d_T(x, z) + d_T(z, y) = 1 + 0 = 1$$
                    \item if $y \in A$ then $$1 = d_T(x, y) \le d_T(x, z) + d_T(z, y) = 0 + 1 = 1$$
                    \item if $z \in A$ then $$0 = d_T(x, y) \le d_T(x, z) + d_T(z, y) = 1 + 1 = 2$$
                \end{itemize}
        \end{itemize}
    \end{proof}

    We observe that, by definition, for any $T$ it holds that $d_T(x, y) = d_{\overline T}(x, y)$. In fact, if $\{x, y\} \in \mathrm{cut}(T)$ then $\abs{T \cap \{x, y\}} = 1$, meaning that exactly one endpoint of $\{x, y\}$ lies in $T$, which must imply that the other endpont lies in $\overline T$. Vice versa, if $\{x, y\} \notin \mathrm{cut}(x, y)$, then either $x, y \in T$ --- and therefore $x, y \notin \overline T \implies \abs{\overline T \cap \{x, y\}} \neq 1$ --- or $x, y \notin T$ --- and therefore $x, y \in \overline T \implies \abs{\overline T \cap \{x, y\}} \neq 1$ as well.

    But why are we discussing cut metrics in the first place? We are going to show that the concept of elementary cut metric is \tit{deeply} related to the concept of \tit{sparsity} previously introduced. First, consider the following definition.

    \begin{frameddefn}{Cut-ratio}
        Given a graph $G$, and a subset $T \subseteq V(G)$, the \tbf{cut-ratio} induced by the elementary cut metric $d_T$ is defined as $$\phi(d_T) := \dfrac{\sum_{xy \in E(G)}{d_T(x, y)}}{\sum_{xy \in [V(G)]^2}{d_T(x, y)}}$$
    \end{frameddefn}

    Through some algebraic manipulation, we see that the cut-ratio of $d_T$ is in fact the sparsity of $T$, as shown in the following proposition.

    \begin{framedprop}[label={sparse prop}]{}
        Given a graph $G$, and a subset $T \subseteq V(G)$, it holds that $\phi(d_T) = \psi(T)$.
    \end{framedprop}
    
    \begin{proof}
        We observe that $$\phi(d_T) := \dfrac{\sum_{ij \in E(G)}{d_T(x, y)}}{\sum_{ij \in [V(G)]^2}{d_T(x, y)}} = \dfrac{\sum_{xy \in E(G)}{\1[x \in T \oplus y \in T]}}{\sum_{xy \in [V(G)]^2}{\1[x \in T \oplus y \in T]}} = \dfrac{\mathrm{cut}(S)}{\abs S \cdot \abs{\overline S}} = \psi(T)$$
    \end{proof}

    We can generalize the concept of elementary cut metric through \tbf{cut metrics}, i.e. linear combinations over a given set of elementary cut metrics, as defined below.

    \begin{frameddefn}{Cut metric}
        Given a graph $G$, a sequence $d_{T_1}, \ldots, d_{T_k}$ of elementary cut metrics on $G$, and $k$ positive real values $\lambda_1, \ldots, \lambda_k > 0$, a \tbf{cut metric} is a function defined as follows $$\funcmap{d}{V(G) \times V(G)}{\R}{(x, y)}{\sum_{i \in [k]}{\lambda_i d_{T_i}(x, y)}}$$
    \end{frameddefn}

    As for elementary cut metrics, it can be easily proven that cut metrics are indeed metrics --- we will omit the proof. Moreover, the following proposition shows that the cut-ratio of a cut metric $d$ is lower bounded by the smallest cut-ratio among the elementary cut metrics that define $d$ itself.

    \begin{framedprop}[label={cut metric prop}]{}
        Given a graph $G$, and a cut metric $d$ defined through $d_{T_1}, \ldots, d_{T_k}$, it holds that $$\phi(d) \ge \min_{j \in [k]}{\phi(d_{T_j})}$$
    \end{framedprop}

    \begin{proof}
        First, consider the following property of sums.

        \claim{
            For any $a_1, \ldots, a_k, b_1, \ldots, b_k > 0$, it holds that $$\dfrac{\sum_{i \in [k]}{a_i}}{\sum_{i \in [k]}{b_i}} \ge \min_{j \in [k]}{\dfrac{a_j}{b_j}}$$
        }{
            We observe that $$\dfrac{\sum_{i \in [k]}{a_i}}{\sum_{i \in [k]}{b_i}} = \dfrac{\sum_{i \in [k]}{\tfrac{b_i}{b_i} \cdot a_i}}{\sum_{i \in [k]}{b_i}} \ge \dfrac{\sum_{i \in [k]}{b_i \cdot \min_{j \in [k]}{\tfrac{a_j}{b_j}}}}{\sum_{i \in [k]}{b_i}} = \min_{j \in [k]}{\dfrac{a_j}{b_j}}$$
        }

        Let $\lambda_1, \ldots, \lambda_k > 0$ be the coefficients that define $d$; then, thanks to the claim, we get
        \begin{equation*}
            \begin{split}
                \phi(d) &= \dfrac{\sum_{xy \in E(G)}{d(x, y)}}{\sum_{xy \in [V(G)]^2}{d(x, y)}} \\
                        &= \dfrac{\sum_{xy \in E(G)}{\sum_{i \in [k]}{\lambda_i d_{T_i}(x, y)}}}{\sum_{xy \in [V(G)]^2}{\sum_{i \in [k]}{\lambda_i d_{T_i}(x, y)}}} \\
                        &= \dfrac{\sum_{i \in [k]}{\sum_{xy \in E(G)}{\lambda_i d_{T_i}(x, y)}}}{\sum_{i \in [k]}{\sum_{xy \in [V(G)]^2}{\lambda_i d_{T_i}(x, y)}}} \\
                        &\ge \min_{j \in [k]}{\dfrac{\sum_{xy \in E(G)}{\lambda_j d_{T_j}(x,y)}}{\sum_{xy \in [V(G)]^2}{\lambda_j d_{T_j}(x, y)}}} \\
                        &= \min_{j \in [k]}{\dfrac{\sum_{xy \in E(G)}{d_{T_j}(x, y)}}{\sum_{xy \in [V(G)]^2}{d_{T_j}(x,y)}}} \\
                        &= \min_{j \in [k]}{\phi(d_{T_j})}
            \end{split}
        \end{equation*}
    \end{proof}

    An important consequence of \cref{sparse prop} and \cref{cut metric prop} is the following corollary, which implies that optimizing over \tbf{the sparsest cut} is equivalent to both optimizing over all \tbf{elementary cut metrics} and \tbf{cut metrics} in general.

    \begin{framedcor}{}
        Given a graph $G$, it holds that $$\min_{T \subseteq V(G)}{\psi(T)} = \min_{T \subseteq V(G)}{\phi(d_T)} = \min_{d \ \mathrm{cut \ metric}}{\phi(d)}$$
    \end{framedcor}

    We observe that the last equality derives from the fact that elementary cut metrics are \tit{trivial} cut metrics.

    \subsection{The $\ell_1$ metric}

    Up until this point, we have explored cut metrics in great detail. In the following section, we are going to shift our focus towards other types of metrics, in patricular the $\ell_\mu$ type of metrics, which are defined as shown below.

    \begin{frameddefn}{$\ell_\mu$ metrics}
        Given $\mu \in \R_{\ge 1}$, and a set $S \subseteq \R^d$ for some $d$, the \tbf{$\ell_\mu$ metric} is a function defined as follows $$\ell_\mu(x, y) = \sqrt[\mu]{\sum_{i = 1}^d{\abs{x_i - y_i}^\mu}}$$ We will denote $\ell_\mu(x, y)$ also as $\abs{x - y}_{\ell_\mu}$.
    \end{frameddefn}

    In particular, we are interested in the $\ell_1$ metric. In fact, consider the following definition.

    \begin{frameddefn}{Isometrical embedding}
        Consider two metrics $\func{d_1}{A \times A}{B}$ and $\func{d_2}{X \times X}{Y}$; we say that $d_1$ is \tbf{isometricaly embedded} into $d_2$ if there is a function $\func{f}{A}{X}$ such that $$d_1(x, y) = d_2(f(x), f(y))$$
    \end{frameddefn}

    \begin{framedlem}{}
        Any cut metric defined over $k$ cuts is isometrically embedded into $\ell_1$ over $\R^k$.
    \end{framedlem}

    \begin{proof}
        Let $d$ be a cut metric defined over $T_1, \ldots, T_k$, i.e. $d(x, y) = \sum_{i \in [k]}{\lambda_i d_{T_i}(x, y)}$ for some $\lambda_1, \ldots, \lambda_k > 0$. For each $u \in V(G)$, let $\overline{x_u} \in \R^k$ be a vector defined as follows: $$\overline{x_u}(i) := \soe{ll}{\lambda_i & u \in T_i \\ 0 & u \notin T_i}$$ where $\overline{x_u}(i)$ is the $i$-th component of $\overline{x_u}$.

        Now, consider the function $$\funcmap{f}{V(G)}{\R^k}{u}{\overline{x_u}}$$ Now, fix $i \in [k]$, and two vertices $u, v \in V(G)$; we observe that
        
        \begin{itemize}
            \item $\abs{\{u,v\} \cap T_i} = 1 \implies (u \in T_i \land v \notin T_i) \lor (u \notin T_i \land v \in T_i)$; hence, we have that $$u \in T_i \land v \notin T_i \implies \overline{x_u}(i) = \lambda_i \land \overline{x_v}(i)= 0 \implies \abs{\overline{x_u}(i) - \overline{x_v}(i)} = 1$$ and also that $$u \notin T_i \land v \in T_i \implies \overline{x_u}(i) = 0 \land \overline{x_v} (i) = \lambda_i \implies \abs{\overline{x_u}(i) - \overline{x_v}(i)} = 1$$
            \item $\abs{\{u, v\} \cap T_i} = 0 \implies u, v \in T_i \lor u, v \notin T_i$; hence, we have that $$u, v \in T_i \implies \overline{x_u}(i) = \overline{x_v}(i) = \lambda_i \implies \abs{\overline{x_u}(i) - \overline{x_v}(i)} = 0$$ and also that $$u, v \notin T_i \implies \overline{x_u}(i) = \overline{x_v}(i) = 0 \implies \abs{\overline{x_u}(i) - \overline{x_v}(i)} = 0$$
        \end{itemize}

        This means that for any $i \in [k]$ and $u, v \in V(G)$, it holds that $$\1 \sbk{\abs{\{u, v\} \cap T_i} = 1} = \abs{\overline{x_u}(i)- \overline{x_v}(i)}$$ Therefore, we obtain the following
        \begin{equation*}
            \begin{split}
                d(u, v) &= \sum_{i \in [k]}{\lambda_i d_{T_i}(u, v)} \\
                        &= \sum_{i \in [k]}{\lambda_i \cdot \1 \sbk{\abs{\{u, v\} \cap T_i} = 1}} \\
                        &= \sum_{i \in [k]}{\abs{\overline{x_u}(i) - \overline{x_v}(i)}} \\
                        &= \abs{\overline{x_u} - \overline{x_v}}_{\ell_1} \\
            \end{split}
        \end{equation*}
        meaning that $d$ is indeed isometrically embedded into $\ell_1$ over $\R^k$.
    \end{proof}

    \begin{framedlem}[label={metric lemma}]{}
        For any $d$, and any set $X \underset{fin}{\subseteq} \R^d$, the $\ell_1$ metric over $X$ is isometrically embedded into a cut metric defined over $d \cdot(\abs x - 1)$ cuts.
    \end{framedlem}

    \begin{proof}
        We prove the statement for $d =1$, and then extend the proof to all other values of $d$.

        Let $X \underset{fin}{\subseteq} \R^d$, i.e. $X = \{x_1, \ldots, x_n\}$ for some $n \in \N$; moreover, let $\func{\pi}{[n]}{[n]}$ be a permutation of the indices ordering the $n$ elements of $X$ such that they are ordered in \tit{ascending order} $$x_{\pi(1)} \le \ldots \le x_{\pi(n)}$$ For each $j \in [n - 1]$, let $S_j := \{\pi(1), \ldots, \pi(j)\}$; by construction, we have that $$\forall j \in [n - 1] \quad S_1 \subseteq \ldots \subseteq S_j$$ Moreover, for each $j \in [n - 1]$ set $\lambda_j := x_{\pi(j + 1)} - x_{\pi(j)}$; since the indices order the elements of $X$ in ascending order, we know that $\lambda_j \ge 0$ for all $j \in [n - 1]$.

        Define $d$ to be the cut metric over the cuts $S_1, \ldots, S_{n - 1}$ with coefficients $\lambda_1, \ldots, \lambda_{n - 1}$; hence, for each $i$ and $j$ such that $i < j$ we get that
        \begin{equation*}
            \begin{split}
                d(x_{\pi(i)}, x_{\pi(j)}) &= \sum_{t \in [j]}{\lambda_t d_{S_t}(x_{\pi(i)}, x_{\pi(j)})} \\
                                          &= \sum_{t \in [j]}{\lambda_t \cdot \1 \sbk{\abs{\{x_{\pi(i)}, x_{\pi(j)}\} \cap S_t} = 1}} \\
                                          &= \sum_{i \le t < j}{\lambda_t} \\
                                          &= (x_{\pi(j)} - x_{\pi(j - 1)}) + (x_{\pi(j - 1)} - x_{\pi(j - 2)}) + \ldots + (x_{\pi(i + 1)} - x_{\pi(i)}) \\
                                          &= x_{\pi(j)} - x_{\pi(i)} \\
                                          &= \abs{x_{\pi(j)} - x_{\pi(i)}}_{\ell_1}
            \end{split}
        \end{equation*}
        Lastly, since metrics are symmetric, we conclude that $$\forall i,j \quad d(x_{\pi(i)}, x_{\pi(j)}) = \ell_1(x_{\pi(1)}, x_{\pi(j)})$$ which means that for $d=1$ finite $\ell_1$ metrics can be embedded into a cut metric with $\abs X - 1$ cuts. Now, for any dimension $d > 0$, finite $\ell_1$ metrics can be embedded into a cut metric that is the sum of each 1-dimensional embedding, meaning that $\ell_1$ can be embedded into a cut metric with $d \cdot(\abs X - 1)$ cuts.
    \end{proof}

    These two lemmas further improve our optimization equalities: optimizing over a cut metric is equal to optimizing over an \tbf{$\ell_1$ metric}.

    \begin{framedcor}[label={metric cor}]{}
        Given a graph $G$, it holds that $$\min_{T \subseteq V(G)}{\psi(T)} = \min_{T \subseteq V(G)}{\phi(d_T)} = \min_{d \ \mathrm{cut \ metric}}{\phi(d)} = \min_{d \ \ell_1 \ \mathrm{metric}}{\phi(d)}$$
    \end{framedcor}

    \section{Metric relaxations and distortions}

    \subsection{Metric relaxations}

    Consider the following program:

    \begin{figure}[H]
        \centering
        \[\begin{array}{ccl}
            \qquad\qquad\quad
            & \max \; \dfrac{\sum_{ij \in E(G)}{d_{ij}}}{\sum_{ij \in [V(G)]^2}{d_{ij}}} \\\\
            & \sum\limits_{S \subseteq V(G)}{\lambda_Sd_S(i, j)}  = d_ij & \forall ij \in [V(G)]^2 \\
            & \lambda \in \R^{2^n} \\
            & d \in \R^{\abs{\binom{n}{2}}} \\
        \end{array}\]
        \caption{Non-linear program for SC.}
    \end{figure}

    In this \tit{non}-linear program, the variables $d_{ij}$ and $\lambda_S$ --- as their names suggest --- represent the cut values for the optimal cut metric $d^*$ described by the optimal solution, which corresponds to $\phi(d^*)$. Note that the values $d_S(i,j)$ are elementary cut metrics, hence they act as nothing more than a constant coefficient, while we optimize over the values $\lambda_S$ defining the \tit{actual} coefficients of the cut.

    Additionally, the main constraint of the program optimizes over cut metrics, which we proved to be \curlyquotes{isometrically equivalent} to $\ell_1$ metrics in the previous lemmas. Therefore, this program can be viewed as optimizing both over cut metrics and $\ell_1$ metrics. However, the objective function is clearly \tit{non}-linear, hence the program is \tit{non}-linear as well. To fix this, we can normalize the denominator of the objective function since it is shared among all feasible solutions, obtaining the following LP.

    \begin{figure}[H]
        \centering
        \[\begin{array}{ccl}
            \qquad\qquad\quad
            & \max \; \displaystyle \sum_{ij \in E(G)}{d_{ij}} \\\\
            & \sum\limits_{S \subseteq V(G)}{\lambda_Sd_S(i, j)}  = d_ij & \forall ij \in [V(G)]^2 \\
            & \sum\limits_{ij \in [V(G)]^2}{d_ij} = 1 \\
            & \lambda \in \R^{2^n} \\
            & d \in \R^{\abs{\binom{n}{2}}} \\
        \end{array}\]
        \caption{LP for SC.}
    \end{figure}

    By construction, the solution to this LP corresponds to the optimal cut metric $d^*$, and the optimal solution to SC is given by the minimal elementary cut describing $d^*$; therefore, this LP could be used to get an exact solution to SC. However, we cannot employ this LP because it requires an \tit{exponential} number of variables. To solve this issue, consdier the following definition.

    \begin{frameddefn}{Metric distortion}
        Given two metrics $d_1$ and $d_2$ over the same vector space $V$, we say that $d_2$ has a \tbf{distortion} from $d_1$ of at most $\alpha \beta$ if $$\forall x, y \in V \quad \dfrac{d_1(x, y)}{\alpha} \le d_2(x, y) \le \beta d_1(x, y)$$
    \end{frameddefn}

    Metric distortion describes the concept of \tit{similarity} between measures. In 1985 \textcite{bourgain} proved that any finite metric can be isometrically embedded into an $\ell_1$ metric, up to some distortion factor.

    \begin{framedthm}[label={bourgain}]{Bourgain's theorem (computational version)}
        Any metric $d$ on $n$ points can be isometrically embedded in time $n^O(1)$ into an $\ell_1$ metric on $\R^d$, where $d = O(\log^3 n)$ and distortion factor $O(\log n)$.
    \end{framedthm}

    Thanks to this result, \textcite{leightonrao} were able to provide an algorithm that yields an $O(\log n)$-approximation to SC by using the following \tbf{metric LP relaxation}: instead of optimizing over $\ell_1$ metrics, they constructed a relaxation that optimizes over \tit{all} metrics --- introducing the approximation factor of $O(\log n)$, which is precisely the distortion factor.

    \begin{figure}[H]
        \centering
        \[\begin{array}{ccl}
            \qquad\qquad\quad
            & \max \; \displaystyle \sum_{ij \in E(G)}{d_{ij}} \\\\
            & x_{ij} + x{jk} \ge x_{ik} & \forall i, j ,k \in V(G) \\
            & \sum\limits_{ij \in [V(G)]^2}{d_ij} = 1 \\
            & \lambda \in \R^{2^n} \\
            & d \in \R^{\abs{\binom{n}{2}}} \\
        \end{array}\]
        \caption{Metric LP relaxation for SC.}
    \end{figure}

    \begin{framedalgo}{Leighton-Rao algorithm}
        Given a graph $G$, the algorithm returns a cut of $G$. \\
        \hrule

        \quad
        \begin{algorithmic}[1]
            \Function{LeightonRao}{$G$}
                \State $d := \mathrm{LP}_\mathrm{metric}(G)$
                \State $d' := \textsc{to-}\ell_1\textsc{-metric}(d)$ \Comment{apply \cref{bourgain}}
                \State $d'' := \textsc{to-cut-metric}(d')$ \Comment{apply \cref{metric lemma}}
                \State Let $T_1, \ldots, T_k$ be the $k$ cuts of $d''$
                \State \tbf{return} $\displaystyle S \in \argmin_{i \in [k]}{\phi(T_i)}$
            \EndFunction
        \end{algorithmic}
    \end{framedalgo}
    
    Thanks to \cref{metric cor} and \cref{bourgain}, we obtain the following result.

    \begin{framedthm}{\textsc{LeightonRao}'s approximation ratio}
        Given a graph $G$, and an optimal solution $S^*$ to SC on $G$, let $S := \textsc{LeightonRao}(G)$. Then, it holds that $$\psi(S) \le O(\log n) \cdot \psi(S^*)$$
    \end{framedthm}
    
    In recent years \textcite{arora} were able to improve the approximation ratio up to $O(\sqrt{\log n})$ through the following SDP.

    \begin{figure}[H]
        \centering
        \[\begin{array}{ccl}
            \qquad\qquad\quad
            & \max \; \displaystyle \sum_{ij \in E(G)}{\abk{x_i,x_i} - 2 \abk{x_i,x_j} + \abk{x_j,x_j}} \\\\
            & \abk{x_i, x_j} - \abk{x_j, x_j} + \abk{x_j, x_k} \le \abk{x_i, x_k} & \forall i, j ,k \in V(G) \\
            & \sum\limits_{ij \in [V(G)]^2}{\abk{x_i,x_i} - 2 \abk{x_i,x_j} + \abk{x_j,x_j}} = 1 \\
            & \lambda \in \R^{2^n} \\
            & d \in \R^{\abs{\binom{n}{2}}} \\
        \end{array}\]
        \caption{$\ell_2^2$ metric SDP relaxation for SC.}
    \end{figure}

    The idea is similar to that of the Leighton-Rao algorithm: instead of relaxing to every metric, they relax the problem to \tbf{squared $\ell_2$ metrics}. In particular, they observed that
    \begin{equation*}
        \begin{split}
            \ell_2^2(x, y) &= \sum_{i \in [n]}{\abs{x_i - y_i}^2} \\
                           &= \sum_{i \in [n]}{\rbk{x_i^2 - 2x_iy_i + y_i^2}} \\
                           &= \sum_{i \in [n]}{x_i^2 - 2\sum_{i \in [n]}{x_iy_i} + \sum_{i \in [n]}{y_i^2}} \\
                           &= \abk{x,x} - 2 \abk{x,y} + \abk{y,y}
        \end{split}
    \end{equation*}
    Note that $\ell_2^2$ does not always respect the \tit{triangle inequality}. For instance, in $\R$ we have that $\ell_2^2(-1, 0) = \ell_2(0, 1) = 1$, but $\ell_2^2(-1, 1) = 4 > 1$. Hence, the triangle inequality is forced by second constraint of the SDP itself, in fact: $$\ell_2^2(x, z) + \ell_2^2(z, y) \le \ell_2^2(x, y) \implies$$ $$(\abk{x_i, x_i} - 2\abk{x_i, x_j} + \abk{x_j, x_j}) + (\abk{x_j, x_j} - 2\abk{x_j , x_k} + \abk{x_k, x_k}) \ge \abk{x_i, x_i} - 2 \abk{x_i, x_k} + \abk{x_k, x_k}$$ $$\implies -2 \abk{x_i, x_j} + 2\abk{x_j , x_j} - 2 \abk{x_j , x_k } \ge -2 \abk{x_i , x_k}$$ $$\abk{x_i, x_j} - \abk{x_j , x_j} + \abk{x_j , x_k} \le \abk{x_i, x_k}$$

    \begin{framedprop}{}
        Any $\ell_2^2$ metric on $n$ points can be isometrically embedded in time $n^O(1)$ into an $\ell_1$ metric on $\R^d$ with distortion factor $O(\sqrt{\log n})$.
    \end{framedprop}

    \begin{proof}
        Omitted.
    \end{proof}

    This $O(\sqrt{\log n})$ is the current best known approximation for SC. Moreover, SC cannot be expressed in temrs of CSP --- recall that the above SDP is a \tit{relaxation} for the minimal $\ell_2^2$ problem. Furthermore, even if this approximation ratio is believed to be the best possible one, the UGC only implies that there are no constant factor approximations for SC, meaning that the non-constant ratio could be proved to be lower.

    \subsection{Shortest path metric}

    Metric distortion has showed to be a useful tool for various approximation algorithms. However, in some instances there is a \tit{lower bound} on the distortion required in order to embed a metric into another. For instance, consider the following metric.

    \begin{frameddefn}{Shortest path metric}
        Given a graph $G$, the \tbf{shortest path metric} is a function $\func{d}{V(G) \times V(G)}{\R}$ defined as follows

        $$d_G(x, y) = \abs{\{e \in E(P) \mid P \ \mathrm{shortest} \ u \to v \ \mathrm{path}\}}$$
    \end{frameddefn}

    It can be easily proven that this is indeed a metric --- we will omit the proof.

    We are going to show that the shortest path metric cannot be embedded into $\ell_2$ over $\R^d$ without distortion. In particular, we will show the case for the cycle graph $C_4$. Consider the embedding $\func{f}{V(C_4)}{\R^2}$ such that $$f(1) = \rbk{-\dfrac{1}{2}; \dfrac{1}{2}} \quad f(2) = \rbk{\dfrac{1}{2}; \dfrac{1}{2}} \quad f(3) =\rbk{\dfrac{1}{2}; -\dfrac{1}{2}} \quad f(4) = \rbk{-\dfrac{1}{2}; -\dfrac{1}{2}}$$ It is easy to see that this embedding is isometrical from $d_{C_4}$ to $\ell_1$, since all distances are preserved. However, for the $\ell_2$ metrc the diagonals are \tit{not} isometric, in fact $$2 = d_{C_4}(1, 3) \neq \ell_2(f(1), f(3)) = \ell_2 \rbk{\rbk{-\dfrac{1}{2}; \dfrac{1}{2}},\rbk{\dfrac{1}{2}; -\dfrac{1}{2}}} = \sqrt 2$$ In particular, this embedding induces a distortion factor of at most $\sqrt 2$. Furthermore, thanks to the following lemma we prove that this distortion factor is actually the \tit{best} that we can achieve.

    \begin{framedlem}{Shortest Diagonal lemma}
        For any $y_1, y_2, y_3, y_4 \in \R^4$ it holds that $$\ell_2^2(y_1, y_3) + \ell_2^2(y_2, y_4) \le \ell_2^2(y_1, y_2) + \ell_2^2(y_2, y_3) + \ell_2^2(y_3, y_4) + \ell_2^2(y_4, y_1)$$
    \end{framedlem}

    \begin{proof}
        We will employ a technique called \tit{sum of squares proof}: the idea is to show that one equation made of sums of squares is equivalent to the square of an other equation, implying that the first one must be always non-negative. We observe that finding the second equation to use as \curlyquotes{comparison} is \tit{very} hard.

        We will prove the lemma coordinate-by-coordinate: fix $t \in [d]$, and for each $i \in [4]$ let $z_i := y_i(t)$ be the $t$-th coordinate of $y_i$.

        \claim{
            For all $t \in [d]$ it holds that \centeredeq{0.9}{$(z_1 - z_4) ^2 + (z_2 - z_3) ^2 + (z_3 - z_4) ^2 + (z_4 - z_1) ^ 2 - (z_1 - z_3) ^2 - (z_2 - z_4)^2 = (z_1 - z_2 + z_3 - z_4)^2$}
        }{
            It can be proven by expanding both the left and right hand side of the quation.
        }

        Because of the claim, we get that $$(z_1 - z_4) ^2 + (z_2 - z_3) ^2 + (z_3 - z_4) ^2 + (z_4 - z_1) ^ 2 - (z_1 - z_3) ^2 - (z_2 - z_4)^2 = (z_1 - z_2 + z_3 - z_4)^2 \ge 0$$ since the LHS is a square. Now, recalling that $\ell_2^2(y_i, y_j) := \sum_{t \in [d]}{(y_t(i) - y_t(j))^2}$, we get that $$\ell_2^2(y_1, y_2) + \ell_2^2(y_2, y_3) + \ell_2^2(y_3, y_4) + \ell_2^2(y_4, y_1) - \ell_2^2(y_1, y_3) - \ell_2^2(y_2, y_4) \ge 0$$
    \end{proof}

    \begin{framedprop}{}
        Any embedding of the shortest path metric $d_{C_4}$ into $\ell_2$ over $\R^d$, for any $d \in \N_{\ge 1}$, has a distortion factor of at least $\sqrt 2$.
    \end{framedprop}

    \begin{proof}
        Fix $d \in \N_{\ge 1}$, and consider an embedding $\func{f}{V(C_4)}{\R^d}$. Without loss of generality, we may assume that $f(i) = x_i$ for each $i \in [4]$. Let $M$ be the maximum $\ell_2$ edge length over all the points embedded into $\R^d$, in other words $$M_{\ell_2} := \max(\ell_2(x_1, x_2), \ell_2(x_2, x_3), \ell_2(x_3, x_4), \ell_2(x_4, x_1))$$ and let $m$ be the minimum $\ell_2$ diagonal length over all the points embedded into $\R^d$, in other words $$m_{\ell_2} := \min(\ell_2(x_1, x_3), \ell_2(x_2, x_4))$$ Thanks to the Shortest Diagonal lemma, we have that
        \begin{equation*}
            \begin{split}
                2m_{\ell_2}^2 &\le \ell_2^2(y_1, y_3) + \ell_2^2(y_2, y_4) \\
                     &\le \ell_2^2(y_1, y_2) + \ell_2^2(y_2, y_3) + \ell_2^2(y_3, y_4) + \ell_2^2(y_4, y_1) \\
                     &\le 4M_{\ell_2}^2
            \end{split}
        \end{equation*}
        implying that $2m_{\ell_2}^2 \le 4M_{\ell_2}^2 \iff m_{\ell_2} \le \sqrt 2 M_{\ell_2}$. Now, note that the maximum edge length $M_{C_4}$ w.r.t. $d_{C_4}$ is 1, and the minimum diagonal length $m_{C_4}$ w.r.t. $d_{C_4}$ is 2, which means that $m_{C_4} = 2 M_{C_4}$. Therefore, the embedding must have applied a distortion factor of at least $\sqrt 2$.
    \end{proof}

    \section{Exercises}
    
    \begin{framedprob}{}
        Let $d$ be a cut metric. Prove that $d$ satisfies the triangle inequality.
    \end{framedprob}

    \solution{
        Let $d$ be defined over $T_1, \ldots, T_k$ and $\lambda_1, \ldots, \lambda_k >0$, i.e. $$d(x, y) := \sum_{i \in [k]}{\lambda_id_{T_i}(x, y)}$$ Fix $x$, $y$ and $z$; we observe that $$d(x, y) + d(y, z) = \sum_{i \in [k]}{\lambda_id_{T_i}(x, y)} + \sum_{i \in [k]}{\lambda_id_{T_i}(y, z)} = \sum_{i \in [k]}{\lambda_i \rbk {d_{T_i}(x, y) + d_{T_i}(y, z)}}$$ and since $d_{T_1}, \ldots, d_{T_k}$ are elementary cut metrics, they must satisfy the triangle inequality, therefore $$\sum_{i \in [k]}{\lambda_i \rbk {d_{T_i}(x, y) + d_{T_i}(y, z)}} \ge \sum_{i \in [k]}{\lambda_i d_{T_i}(x, z)} = d(x, z)$$ which proves that $d$ indeed satisfies the triangle inequality.
    }

    \begin{framedprob}{}
        Let $G = (V, E)$ be the graph with node set $V = [2n]$ for $n \in \N$, and edge set $$E = \{\{i, i + 1\} \mid 1 \le i \le 2n - 1\} \cup \{\{2n, 1\}\}$$ meaning that $G$ is a cycle on $2n$ nodes. Let $d_G$ be the shortest path metric on $G$. Prove that $d_G$ can be embedded isometrically into $\ell_1$.
    \end{framedprob}

    \solution{
        First, we observe that the shortest path metric on a $2n$-long cycle graph is defined as follows: $$\forall j, k \in [2n] \quad d_G(j, k) = \soe{ll}{\abs{k - j} & \abs{k - j} \le n \\ 2n - \abs{k - j} & \abs{k - j} > n}$$ Moreover, we observe that $\ell_1$ defined on two binary vectors $x, y \in \{0, 1\}^h$ is equivalent to their \href{https://en.wikipedia.org/wiki/Hamming_distance}{Hamming distance}, i.e. $$\ell_1(x, y) = d_H(x, y) = \abs{\{i : x_i \neq y_i\}}$$ and in particular $$\forall j, k \in [2n] \quad \ell_1(f(j), f(k)) = d_H(f(j), f(k))$$

        Now, consider the function $\func{f}{V(C_{2n})}{\R^n}$ defined as follows: $$\forall i \in [2n] \quad f(i) = \soe{ll}{0^{n - i + 1}1^{i - 1} & i \le n \\ 1^{2n - i + 1}0^{i - 1 - n} & i > n}$$ where the notation $0^a1^b$ stands for the vector $(\underbrace{0, \ldots, 0}_{a \ \mathrm{values}}, \underbrace{1, \ldots, 1}_{b \ \mathrm{values}}) \in \R^{a + b}$. At first, this definition of $f$ may seem counterintuitive, but if we consider each possible $f(i)$ the following nice pattern of \curlyquotes{sliding window} emerges:

    \begin{center}
        \begin{tabular}{c|c}
            \hline
            $i$ & $f(i)$ \\
            \hline\hline
            1 & 0000 \\
            \hline
            2 & 0001 \\ 
            \hline
            3 & 0011 \\
            \hline
            4 & 0111 \\
            \hline
            5 & 1111 \\
            \hline
            6 & 1110 \\
            \hline
            7 & 1100 \\
            \hline
            8 & 1000 \\
            \hline
        \end{tabular}
    \end{center}

        Finally, fix $j, k \in [2n]$, and without loss of generality suppose that $j < k$ -- in particular, this implies that $\abs{k - j} = k - j$. We have some cases to handle.

        \begin{itemize}
            \item $j, k \le n$; therefore, we have that
                \begin{itemize}
                    \item $j, k \le n \implies k - j \le n \implies d_G(j, k) = k - j$
                    \item $j < k \implies n - j + 1 > n - k + 1 \land j - 1 < k - 1$
                \end{itemize}
                and thus, to evaluate the Hamming distance between $f(j)$ and $f(k)$ we need to consider the \curlyquotes{middle} section in which the two binary vectors may differ
                \begin{equation*}
                    \begin{split}
                        \ell_1(f(j), f(k)) &= d_H(0^{n - j + 1}1^{j - 1}, 0^{n - k + 1}1^{k - 1}) \\
                                           &= [n - (j - 1)] - [(n - k + 1) + 1] + 1 \\
                                           &= [n - j + 1] - [n - k + 2] + 1 \\
                                           &= k - j \\
                                           &= d_G(j, k)
                    \end{split}
                \end{equation*}

            \item $j, k > n$; therefore, we have that
                \begin{itemize}
                    \item $j, k > n \implies k - j < n \implies d_G(j, k) = k - j$
                    \item $j < k \implies 2n - j + 1 > 2n - k + 1 \land j - 1 - n < k - 1 - n$
                \end{itemize}
                hence again, we just need to consider the \curlyquotes{middle} section in this case too
                \begin{equation*}
                    \begin{split}
                        \ell_1(f(j), f(k)) &= d_H(1^{2n - j + 1}0^{j - 1- n}, 1^{2n - k + 1}, 0^{k - 1- n}) \\
                                           &= [n - (k - 1- n)] - [(2n - j + 1) + 1] + 1 \\
                                           &= [n - k + 1 + n] - [2n - j + 2] + 1 \\
                                           &= k - j \\
                                           &= d_G(j, k)
                    \end{split}
                \end{equation*}

            \item $j \le n$ and $k < n$. We have two subcases
                \begin{enumerate}
                    \item $k - j \le n$; therefore, we have that
                        \begin{itemize}
                            \item $k - j \le n \implies d_G(j, k) = k - j$
                            \item $k - j \le n \implies n - j + 1 \le 2n - k + 1 \land j - 1 \ge k - n - 1$
                        \end{itemize}
                        and thus, in this case to evaluate the Hamming distance between $f(j)$ and $f(k)$ we need to consider the sections in which the two vectors can be 0 --- since they can only coincide when they are both 1
                        \begin{equation*}
                            \begin{split}
                                \ell_1(f(j), f(k)) &= d_H(0^{n - j + 1}1^{j - 1}, 1^{2n - k + 1}0^{k - 1- n}) \\
                                                   &= (n - j + 1) + (k - 1 - n) \\
                                                   &= k - j \\
                                                   &= d_G(j, k)
                            \end{split}
                        \end{equation*}
                    \item $k - j > n$; therefore, we have that
                        \begin{itemize}
                            \item $k - j > n \implies d_G(j, k) = 2n - k + j$
                            \item $k - j > n \implies n - j + 1 > 2n - k + 1 \land j - 1 < k - n - 1$
                        \end{itemize}
                        and thus, in this case to evaluate the Hamming distance between $f(j)$ and $f(k)$ instead we need to consider the sections in which the two vectors can be 1 --- since they can only coincide when they are both 0
                        \begin{equation*}
                            \begin{split}
                                \ell_1(f(j), f(k)) &= d_H(0^{n - j + 1}1^{j - 1}, 1^{2n - k + 1}0^{k - 1- n}) \\
                                                   &= (j - 1) + (2n - k + 1) \\
                                                   &= 2n - j + k \\
                                                   &= d_G(j, k)
                            \end{split}
                        \end{equation*}
                \end{enumerate}
        \end{itemize}
    }

    \chapter{Submodular optimization}

    \tbf{Submodular optimization} plays a central role in \href{https://en.wikipedia.org/wiki/Discrete_optimization}{discrete optimization}, particularly in algorithmic settings where efficiency and approximation guarantees are critical. Submodular functions model a \tit{diminishing returns} property that arises naturally in many computational problems, such as influence maximization, data summarization, and sensor placement. But before introducing submodular optimization in detail, we will discuss the following problem.

    \begin{frameddefn}{Max Cover problem}
        The \tbf{Max Cover} (MC) problem is defined as follows: given a \tit{universe} (or \tit{ground}) set $\mathcal U = [n]$, a collection of sets $C = \{S_1, \ldots, S_m \}$ such that $S_i \subseteq \mathcal U$, and an integer $k \ge 1$, determine the sub-collection $S \subseteq C$ such that $\abs S = k$ that maximizes $\abs{\bigcup_{S_j \in S}{S_j}}$.
    \end{frameddefn}

    In other words, we are asked to determine the sub-collection of the given $C$ of $k$ sets that covers as many elements of $\mathcal U$ as possible. For instance, given $\mathcal U = [7]$, $S_1 = \{1, 2\}$, $S_2 = \{2, 3\}$, $S_3 = \{3, 4, 5\}$, $S_4 = \{5, 6, 7\}$, an optimal solution would be $S = \{S_1, S_3\}$ because $\abs{S_1 \cup S_3} = \abs{\{1, 2, 3, 4, 5\}} = 5$.

    The approximation of MC is \tit{solved}, meaning that the best known approximation ratio of $1 - \tfrac{1}{e}$ has been proven to be tight under \NPclass-hardness. Such approximation ratio can be achieved through the following greedy algorithm.

    \begin{framedalgo}{$\rbk{1 - \frac{1}{e}}$-approximation for MC}
        Given an instance of MC $(\mathcal U, C, k)$, the algorithm returns a $\rbk{1 - \tfrac{1}{e}}$-approximation for the instance. \\
        \hrule

        \quad
        \begin{algorithmic}[1]
            \Function{ApproxMaxCover}{$\mathcal U$, $C$, $k$}
                \State $S := \varnothing$
                \State $T_0 := \varnothing$
                \For{$i \in [k]$}
                    \State $\displaystyle S_i \in \argmax_{S_j \in C - S}{\abs{S_j - T_{i - 1}}}$ \Comment{$S_i$ maximizes the number of \curlyquotes{new} elements}
                    \State $S = S \cup \{S_i\}$
                    \State $T_i = T_{i - 1} \cup S_i$
                \EndFor
                \State \tbf{return} $S$
            \EndFunction
        \end{algorithmic}
    \end{framedalgo}

    \begin{framedthm}[label={mc approx ratio}]{\textsc{ApproxMaxCover}'s approximation ratio}
        Given an instance $(U, C, k)$ of MC, and an optimal solution $S^*$ to MC on $(\mathcal U, C, k)$, let $S := \textsc{ApproxMaxCover}(\mathcal U, C, k)$. Then, it holds that $$\abs{\bigcup_{S_j \in S}} \ge \rbk{1 - \dfrac{1}{e}}\abs{\bigcup_{S_j \in S^*}{S_j}}$$
    \end{framedthm}

    \begin{proof}
        Consider an instance $(\mathcal U, C, k)$ of MC, and an optimal solution $S^*$ to it; moreover, let $\displaystyle X^* := \bigcup_{S_j^* \in S^*}{S_j^*}$ be the set of elements of $\mathcal U$ covered by $S^*$.

        Let $S := \textsc{ApproxMaxCover}(\mathcal U, C, k)$; hence, we have that $T_i = \bigcup_{j = 1}^i{S_j}$. Additionally, let $t_i := \abs{T_i}$ and $\mu_i := \abs{X^*} - t_i$. Hence, we have that
        
        \begin{itemize}
            \item $t_0 = \abs{T_0} = \abs{\varnothing} = 0$
            \item $t_k = \abs{T_k} = \abs{\bigcup_{S_j \in S}{S_j}}$ is the value of $S$
            \item $\mu_0 = \abs{X^*} - t_0 = \abs{X^*} - 0 = \abs{X^*}$ is the value of the optimal solution $S^*$
        \end{itemize}

        Therefore, to prove the approximation ratio of the statement it suffices to show that $t_k \ge \rbk{1 - \tfrac{1}{e}} \mu_0$.

        \claim{
            $\forall i \in [k] \quad \abs{S_i - T_{i - 1}} \ge \dfrac{\mu_{i - 1}}{k}$
        }{
            Since $S^*$ is an optimal solution that covers $\abs{X^*}$ elements of $\mathcal U$, there must be at least one set $S_j^* \in S^*$ that covers at least \todo{i'm missing something here} % $\tfrac{\mu_{i - 1}}{k} = \tfrac{}{}$
        }

        \claim{
            $\forall i \in [0, k] \quad \mu_i \le \rbk{1 - \dfrac{1}{k}}^i\abs{X^*}$
        }{
            We will prove the claim by induction on $i$. In particular, if $i = 0$, we have that $$\mu_0 = \abs{X^*} \rbk{1 - \dfrac{1}{k}}^0 \abs{X^*} = 1 \cdot \abs{X^*}$$ hence the base case trivially holds. Then, assuming the inductive hypothesis, we can prove the inductive step as follows \todo{non capisco (TODO dentro allo split)}
            \begin{equation*}
                \begin{split}
                    \mu_{i + 1} &= \abs{X^*} - t_{i + 1} \\
                                &= \abs{X^*} - \abs{T_{i + 1}} \\
                                % &= \abs{X^*} - \abs{\bigcup_{j = 1}^{i + 1}{S_j}} \\
                                &= \abs{X^*} - TODO \\
                                &= \abs{X^*} - \abs{T_i} - \abs{S_{i + 1} - T_i} \\
                                &\le \mu_i - \dfrac{\mu_i}{k} \quad \quad (\mathrm{for \ the \ previous \ claim}) \\
                                &= \rbk{1 - \dfrac{1}{k}}\mu_i \\
                                &\le \rbk{1 - \dfrac{1}{k}} \rbk{1 - \dfrac{1}{k}}^i \abs{X^*} \quad \quad (\mathrm{by \ inductive \ hypothesis \ on} \ \mu_i) \\
                                &= \rbk{1 - \dfrac{1}{k}}^{i + 1}\abs{X^*}
                \end{split}
            \end{equation*}
        }

        Thanks to this claim, recalling that for any $k \ge 1$ it holds that $\rbk{1 - \tfrac{1}{k}}^k \le e^{-1}$, we get $$\mu_k \le \rbk{1 - \dfrac{1}{k}}^k \abs{X^*} \le \dfrac{1}{e} \abs{X^*}$$ Lastly, since $\mu_k = \abs{X^*} - \abs{T_k}$, we get that $$\abs{X^*} - \abs{T_k} = \mu_k \le \dfrac{1}{e} \abs{X^*} \iff \abs{T_k} \ge \rbk{1 - \dfrac{1}{e}} \abs{X^*} \iff t_k \ge \rbk{1 - \dfrac{1}{e}} \mu_0$$
    \end{proof}

    \section{Submodular functions}

    We are now ready to introduce \tbf{submodular functions}, which play a central role in \tit{submodular optimization} that we introduced at the beginnig of the chapter.

    \begin{frameddefn}{Modular functions}
        Given $n \in \N$, let $\func{f}{\powerset([n])}{\R}$ be a function; given two sets $S, T \subseteq [n]$, we say that $f$ is

        \begin{itemize}
            \item a \tbf{submodular function} if $f(S) + f(T) \ge f(S \cup T) + f(S \cap T)$
            \item a \tbf{modular function} if $f(S) + f(T) = f(S \cup T) + f(S \cap T)$
            \item a \tbf{supermodular function} if $f(S) + f(T) \le f(S \cup T) + f(S \cap T)$
        \end{itemize}
    \end{frameddefn}

    For instance, for any $n \in \N$ the \tit{cardinality} function, i.e. $f(S) = \abs S$ is modular, because $\forall S, T \subseteq [n] \quad f(S \cup T) = f(S) + f(T) - f(S \cap T)$ by the \href{https://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle}{inclusion-exclusion principle}. We observe that submodular and supermodular functions are modular, therefore any property that holds for modular functions is true for the other two types as well.
    
    The main property of modular functions is the property to \curlyquotes{know the full description of the function} through at most $n + 1$ values. For example, if $f$ is modular, then the value of $f(\{1, 2, 3\})$ can be computed through $f(\varnothing)$, $f(\{1\})$, $f(\{2\})$ and $f(\{3\})$ as follows $$f(\{1, 2, 3\}) = f(\{1, 2\}) + f(\{3\})- f(\varnothing) = (f(\{1\}) + f(\{2\}) - f(\varnothing)) + f(\{3\}) - f(\varnothing)$$

    \begin{framedprop}{}
        For any $n \in \N$, the function $\func{f}{\powerset([n])}{\R}$ is modular if and only if there exist $x, w_1, \ldots, w_n$ such that for all $S \subseteq [n]$ it holds that $f(S) = z + \sum_{i \in S}{w_i}$.
    \end{framedprop}

    \begin{proof}
        Let $f$ be a modular function defined on $n \in \N$, and fix a set $S \subseteq [n]$; in particular, let $S = \{1, \ldots, k\}$. By modularity of $f$, we have that
        \begin{equation*}
            \begin{split}
                f(S) &= f(\{1, \ldots, k\}) \\
                     &= f(\{2, \ldots, k\}) + f(\{1\}) - f(\varnothing) \\
                     &= (f(\{3, \ldots, k\}) + f(\{2\}) - f(\varnothing)) + f(\{1\}) - f(\varnothing) \\
                     &= f(\{k\}) + \sum_{i \in S - \{k\}}{(f(\{i\}) - f(\varnothing))} \\
                     &= f(\varnothing) + \sum_{i \in S}{f(\{i\}) - f(\varnothing)} \\
                     &= z + \sum_{i \in S}{w_i}
            \end{split}
        \end{equation*}
        where $z := f(\varnothing)$ and $w_i := f(\{i\}) - f(\varnothing)$ for all $i \in S$.

        Vice versa, suppose that there are values $z', w_1', \ldots, w_n'$ such that for all $S \subseteq [n]$ it holds that $f(S) = z' + \sum_{i \in S}{w_i'}$. Fix two sets $S, T \subseteq [n]$; then, we have that
        \begin{equation*}
            \begin{split}
                f(S) + f(T) &= \rbk{z' + \sum_{i \in S}{w_i'}} + \rbk{z' + \sum_{i \in T}{w_i'}} \\
                            &= \rbk{z' + \sum_{i \in S \cap T}{w_i'} + \sum_{i \in S - T}{w_i'}} + \rbk{z' + \sum_{i \in S \cap T}{w_i'} + \sum_{i \in T - S}{w_i'}} \\
                            &= \rbk{z' + \sum_{i \in S \cap T}{w_i'}} + \rbk{z' + \sum_{i \in S - T}{w_i'} + \sum_{i \in S \cap T}{w_i'} + \sum_{i \in T - S}{w_i'}} \\
                            &= f(S \cap T) + f(S \cup T)
            \end{split}
        \end{equation*}
        hence $f$ is modular.
    \end{proof}

    The importance of submodularity in optimization stems from its inherent \tbf{diminishing returns} property. In economics, diminishing returns describe the phenomenon where the incremental output of a production process decreases as the quantity of a single input increases, while all other inputs are held constant.

    \begin{frameddefn}{Diminishing returns}
        Given $n \in \N$, let $\func{f}{\powerset([n])}{\R}$; given $S \subseteq [n]$ and $x \in [n] - S$, the \tbf{return} of $x$ on $S$ for $f$ is defined as $$\Delta_f(x \mid S) = f(S \cup \{x\}) - f(S)$$ We say that $f$ has \tbf{diminishing returns} when it holds that $$\forall A \subseteq B \subseteq [n], x \in [n] - B \quad \Delta_f(x \mid A) \ge \Delta_f(x \mid B)$$
    \end{frameddefn}

    \begin{framedthm}{}
        For any $n \in \N$, the function $\func{f}{\powerset([n])}{\R}$ is submodular if and only if it has diminishing returns.
    \end{framedthm}

    \begin{proof}
        Let $\func{f}{\powerset([n])}{\R}$ be a submodular function for some $n \in \N$, and fix $A \subseteq B \subseteq [n]$ and $x \in [n] - B$; by submodularity of $f$, we have that $$f(A \cup \{x\}) + f(B) \ge f(A \cup \{x\} \cup B) + f((A \cup \{x\}) \cap B)$$ Now, since $A \subseteq B$, and $x \notin B$, we have that $$f(A \cup \{x\}) + f(B) \ge f(B \cup \{x\}) + f(A)$$ Lastly, rearranging the terms, we get that $$\Delta_f(x \mid A) := f(A \cup \{x\}) - f(A) \ge f(B \cup \{x\}) - f(B) =: \Delta_f(x \mid B)$$

        TODO \todo{do the other direction}
    \end{proof}

    \subsection{TODO}

    Why did we present the Max Cover problem at the beginning of the chapter? MC is a \tbf{submodular optimization} problem, i.e. the problem can be described in terms of a submodular function.

    \begin{frameddefn}{Covering function}
        Given $t \in \N$, let $\mathcal U = [t]$ be the universe set, and let $C = \{A_1, \ldots, A_n\}$ be a collection of subsets $A_i \subseteq \mathcal U$; the \tbf{covering function} $\func{f}{\powerset([n])}{\R}$ is defined as follows: $$\forall S \subseteq [n] \quad f(S) := \abs{\bigcup_{j \in S}{A_j}}$$
    \end{frameddefn}

    For example, given $t = 4$ and $A_1 = \{1, 2, 4\}$, $A_2 = \{2, 3\}$, $A_3 = \{1, 4\}$ --- hence $n = 3$ and $C = \{A_1, A_2, A_3\}$ --- we have that $$f(\{1, 3\}) = \abs{\bigcup_{j \in \{1, 3\}}{A_j}} = \abs{A_1 \cup A_3} = \abs{\{1, 2, 4\}} = 3$$ and so on. In other words, the set $S$ is used to \tit{index} the sets $A_j \in C$.

    \begin{framedprop}{}
        The covering function is non-negative, monotone increasing and submodular.
    \end{framedprop}

    \begin{proof}
        Given a set $X \subseteq [n]$, note that $$f(X) = \sum_{i \in [t]}{\1\sbk{\exists j \in X \quad i \in A_j}} = \sum_{i \in [t]}{f_i(X)}$$ where for each $i \in [t]$ the subfunction $\func{f_i}{\powerset([n])}{\R}$ is defined precisely as $$f_i(X) := \soe{ll}{1 & \exists j \in X \quad i \in A_j \\ 0 & \mathrm{otherwise}}$$

        \claim{
            For all $i \in [t]$, the subfunction $f_i$ is non-negative, monotone increasing and submodular.
        }{
            Fix $i \in [t]$, and $S, T \subseteq [n]$; by definition, we trivially have that $f_i$ is non-negaitve, therefore $f_i(S) + f_i(T) \ge 0$. Moreover, by definition $f_i$ is also monotone increasing, i.e. $S \subseteq T \implies f_i(S) \le f_i(T)$ --- if $f_i(S) = 0$ then the inequality is always satisfied, and $f_i(S) = 1 \iff \exists j \in S \quad i \in A_j$ but $S \subseteq T \implies j \in T$ hence $f_i(T) = 1$.

            To prove submodularity, we observe that since $S \cap T \subseteq S \cup T$ and $f_i \in \{0, 1\}$, we only have two cases:

            \begin{enumerate}
                \item $f_i(S \cup T) = f_i(S \cap T) = 1$. By definition, we have that \centeredeq{0.9}{$f_i(S \cap T) = 1 \iff \exists j \in S \cap T \quad i \in A_j \implies \exists j \in S \land j \in T \quad i \in A_j \iff f_i(S) = f_i(T) = 1$} Hence, we have that $$f_i(S) + f_i(T) = 1 + 1 = f_i(S \cup T) + f_i(S \cap T)$$
                \item $f_i(S \cup T) = 1$ but $f_i(S \cap T) = 0$. By definition \centeredeq{0.9}{$f_i(S \cup T) \implies \exists j \in S \cup T \quad i \in A_j \implies \exists j \in S \lor j \in T \quad i \in A_j \iff f_i(S) + f_i(T) \ge 1$} hence, we have that $$f_i(S) + f_i(T) \ge 1 = f_i(S \cup T) + f_i(S \cap T)$$
            \end{enumerate}
        }

        Lastly, fix $S, T \subseteq [n]$; because of the claim, we know that $f$ is trivially non-negative and monotone increasing. To prove submodularity, thanks to the claim we have that
        \begin{equation*}
            \begin{split}
                f(S) + f(T) &= \sum_{i \in [t]}{f_i(S)} + \sum_{i \in [t]}{f_i(T)} \\
                            &= \sum_{i \in [t]}{f_i(S) + f_i(T)} \\
                            &\ge \sum_{i \in [t]}{f_i(S \cup T) + f_i(S \cap T)} \\
                            &= \sum_{i \in [t]}{f_i(S \cup T)} + \sum_{i \in [t]}{f_i(S \cap T)} \\
                            &= f(S \cup T) + f(S \cap T)
            \end{split}
        \end{equation*}
    \end{proof}

    Since MC is \NPclass-Hard, this proposition implies that the problem of maximizing non-negative, monotone increasing, and submodular functions over $k$ elements is \NPclass-Hard as well. In fact, the $\rbk{1 - \tfrac{1}{e}}$-approximation for MC that we previously discussed is a specific instance of a more general algorithm, first presented by \textcite{nwf}, that aims at maximizing a Non-Negative and Monotone Increasing (NN-MI) submodular function.

    \begin{framedalgo}{NWF approximation}
        Given an NN-MI submodular function $\func{f}{\powerset(X)}{\R}$, and a value $k \in \N$, the algorithm returns a subset $S \subseteq X$. \\
        \hrule

        \quad
        \begin{algorithmic}[1]
            \Function{NWFapprox}{$f$, $k$}
                \State $S_0 := \varnothing$
                \For{$i \in [k]$}
                    \State $\displaystyle x_i \in \argmax_{x \in X - S_i}{f(S_i \cup \{x\})}$ \Comment{or, equivalently, maximize $\Delta_f(x_{i + 1} \mid S_i)$}
                    \State $S_i := S_{i - 1} \cup \{x_i\}$
                \EndFor
                \State \tbf{return} $S_k$
            \EndFunction
        \end{algorithmic}
    \end{framedalgo}

    \begin{framedthm}{\textsc{NWFapprox}'s approximation ratio}
        Given an NN-MI submodular function $\func{f}{\powerset(X)}{\R}$, a value $k \in \N$, and an optimal set $\displaystyle S^* \in \argmax_{S \in [X]^k}{f(S)}$, let $S := \textsc{NWFapprox}(f, k)$. Then, it holds that $$f(S) \ge \rbk{1 - \dfrac{1}{e}}f(S^*)$$
    \end{framedthm}

    \begin{proof}
        The following proof is a generalization of the proof of \cref{mc approx ratio}, in which we proved MC's approximation ratio. Let $S^* := \{x_1^*, \ldots, x_k^*\}$.

        \claim{
            For any $i \in [k - 1]$ it holds that $f(S^*) \le f(S_i) + k(f(S_{i + 1}) - f(S_i)$.
        }{
            Since $f$ is monotone increasing, we know that $f(S^*) \le f(S_i \cup S^*)$. Now, note that \todo{da finire}
            % \begin{equation*}
            %     \begin{split}
            %         f(S^*) &\le f(S_i \cup S^*) \\
            %                &= f \rbk{S_i \cup \bigcup_{i \in [k]}{x_i^*}} \\
            %                &= TODO
            %     \end{split}
            % \end{equation*}
        }

        TODO \todo{da finire}
    \end{proof}

    \section{Property variations}

    What happens when we \tit{drop} some of the properties of NN-MI submodular functions? For instance, consider the following function.

    \begin{frameddefn}{Cut function}
        Given a graph $G$, the \tbf{cut function} $c$ is a function defined as follows $$\funcmap{c}{\mathcal([n])}{\R}{S}{\abs{\mathrm{cut}(S)}}$$
    \end{frameddefn}

    We observe that this function is trivially non-negative and symmetric, but not monotone increasing: given two nodes $x, y \in V(K_3)$, clearly $\{x\} \subseteq \{x, y\}$ but $c(\{x\}) \ge c(\{x, y\})$. Nonetheless, we can still prove that is is submodular, as described below.

    \begin{framedprop}{}
        The cut function is non-negative, symmetric and submodular.
    \end{framedprop}

    \begin{proof}
        The non-negativity and symmetry of the function trivially hold by definition, hence we are going to prove that it is submodular. Fix two sets $S, T \subseteq [n]$, and define the following 4 sets as follows $$A_S := S - T \quad A_T := T - S \quad A_{S, T} := S \cap T \quad A := [n] - (S \cup T)$$ Now, that $$\mathrm{cut}(S) = \mathrm{cut}(A_S, A_T) \cup \mathrm{cut}(A_S, A) \cup \mathrm{cut}(A_{S, T}, A_T) \cup \mathrm{cut}(A_{S, T}, A)$$ meaning that $$c(S) = \abs{\mathrm{cut}(A_S, A_T)} + \abs{\mathrm{cut}(A_S, A)} + \abs{\mathrm{cut}(A_{S, T}, A_T)} + \abs{\mathrm{cut}(A_{S, T}, A)}$$ Similarly, we get that
        \begin{equation*}
            \begin{split}
                c(T) &= \abs{\mathrm{cut}(A_T, A_S)} + \abs{\mathrm{cut}(A_T, A)} + \abs{\mathrm{cut}(A_{S, T}, A_S)} + \abs{\mathrm{cut}(A_{S, T}, A)} \\
                c(S \cup T) &= \abs{\mathrm{cut}(A_S, A)} + \abs{\mathrm{cut}(A_T, A)} + \abs{\mathrm{cut}(A_{S, T}, A)} \\
                c(S \cap T) &= \abs{\mathrm{cut}(A_{S, T}, A_S)} + \abs{\mathrm{cut}(A_{S, T}, A_T)} + \abs{\mathrm{cut}(A_{S, T}, A)}
            \end{split}
        \end{equation*}
        meaning that $$c(S) + c(T) = c(S \cup T) + c(S \cap T) + 2 \abs{\mathrm{cut}(A_S, A_T)} \ge c(S \cup T) + c(S \cap T)$$ implying that $c$ is indeed submodular.
    \end{proof}

    \textcite{fmv} proved that the $\tfrac{1}{2}$-approximation that we discussed in \cref{max cut alg} can be generalized to \tit{any} function that satisfies the same properties of the cut function.

    \begin{framedlem}{}
        Given $n \in \N$, if $\func{g}{\powerset([n])}{\R}$ is submodular, it holds that $$\forall S \subseteq [n] \quad \avg_{T \subseteq S}{g(T)} \ge \dfrac{g(\varnothing) + g(S)}{2}$$
    \end{framedlem}

    \proofstrind{
        We proceed by strong induction on $\abs S$.
    }{
        If $\abs S = 0$, then $S = \varnothing$, therefore $$g(\varnothing) = \avg_{T \subseteq \varnothing}{g(T)} \ge \dfrac{g(\varnothing) + g(\varnothing)}{2} = g(\varnothing)$$
    }{
        Assume that the statement holds for $\abs S \le i$.
    }{
        We prove the statement for $\abs S = i + 1$. Fix $x \in S$, and let $S' := S - \{x\}$ --- in particular $\abs{S'} = i$. Hence, we get that
        \begin{equation*}
            \begin{split}
                \sum_{T \subseteq S}{g(T)} &= \sum_{T' \subseteq S'}{g(T')} + \sum_{T' \subseteq S'}{g(T' \cup \{x\})} \\
                                           &= \sum_{T' \subseteq S'}{g(T')} + \sum_{T' \subseteq S'}{g(T' \cup \{x\})} + \sum_{T' \subseteq S'}{g(T')} - \sum_{T' \subseteq S'}{g(T')} \\
                                           &= 2\sum_{T' \subseteq S'}{g(T')} + \sum_{T' \subseteq S'}{(g(T' \cup \{x\}) - g(T'))} \\
                                           &= 2\sum_{T' \subseteq S'}{g(T')} + \sum_{\substack{T \subseteq S : \\ x \in T}}{(g(T) - g(T \cap S'))} \\
                                           &\ge 2\sum_{T' \subseteq S'}{g(T')} + \sum_{\substack{T \subseteq S : \\ x \in T}}{(g(T \cup S') - g(S'))} \quad \quad (\mathrm{by \ submodularity \ of} \ g) \\
                                           &= 2\sum_{T' \subseteq S'}{g(T')} + \sum_{\substack{T \subseteq S : \\ x \in T}}{(g(S) - g(S'))} \quad \quad (x \in T \implies T \cup S' = S) \\
                                           &= 2\sum_{T' \subseteq S'}{g(T')} + 2^i(g(S) - g(S')) \quad \quad (\abs{\{T \subseteq S \mid x \in T\}} = 2^i) \\
            \end{split}
        \end{equation*}
        Now, multiplying both sides of the inequality by $2^{- \abs S}$, we get that $$\avg_{T \subseteq S}{g(T)} = 2^{- \abs S}\sum_{T \subseteq S}{g(T)} \ge 2 ^{1 - \abs S}\sum_{T' \subseteq S'}{g(T')} + 2^{i - \abs S}(g(S) - g(S'))$$ and recalling that $\abs S = i + 1$, we have that
        \begin{equation*}
            \begin{split}
                \avg_{T \subseteq S}{g(T)} &\ge 2^{- i}\sum_{T' \subseteq S'}{g(T')} + 2^{-1}(g(S) - g(S')) \\
                                           &= \avg_{T' \subseteq S'}{g(T')} + \dfrac{g(S) - g(S')}{2} \\
                                           &\ge \dfrac{g(\varnothing) + g(S')}{2} + \dfrac{g(S) - g(S')}{2} \quad \quad (\mathrm{by \ the \ strong \ inductive \ hypothesis}) \\
                                           &= \dfrac{g(\varnothing) + g(S)}{2}
            \end{split}
        \end{equation*}
    }

    \begin{framedlem}{}
        Given $n \in \N$, if $\func{g}{\powerset([n])}{\R}$ is submodular, it holds that $$\forall S_1, S_2 \subseteq [n] \quad \avg_{T_1 \subseteq S_1}{\avg_{T_2 \subseteq S_2}{g(T_1 \cup T_2)}} \ge \dfrac{g(\varnothing) + g(S_1) + g(S_2) + g(S_1 \cup S_2)}{4}$$
    \end{framedlem}

    \begin{proof}
        Given $X \subseteq [n]$, for any $T \subseteq [n]$ let $h_X(T) := g(X \cup T)$.

        \claim{
            For any $X \subseteq [n]$, the function $h_X$ is submodular.
        }{
            Fix $A, B \subseteq [n]$; we observe that
            \begin{equation*}
                \begin{split}
                    h_X(A) + h_X(B) &= g(X \cup A) + g(X \cup B) \\
                                    &\ge g(X \cup A \cup B) + g((X \cup A) \cap (X \cup B)) \quad \quad (\mathrm{by \ submodularity \ of} \ g) \\
                                    &= g(X \cup A \cup B) + g(X \cup (A \cap B)) \\
                                    &= h_X(A \cup B) + h_X(A \cap B)
                \end{split}
            \end{equation*}
        }

        Since $h_X$ is submodular, by the previous lemma observe that $$\forall S \subseteq [n] \quad \avg_{T \subseteq S}{h_X(T)} \ge \dfrac{h_X(\varnothing) + h_X(S)}{2} = \dfrac{g(X) + g(X \cup S)}{2}$$ Therefore, for any $S_1, S_2 \subseteq [n]$, we have that
        \begin{equation*}
            \begin{split}
                \avg_{T_1 \subseteq S_1}{\avg_{T_2 \subseteq S_2}{g(T_1 \cup T_2)}} &= \avg_{T_1 \subseteq S_1}{\avg_{T_2 \subseteq S_2}{h_{T_1}(T_2)}} \\
                                                                                    &\ge \avg_{T_1 \subseteq S_1}{\dfrac{g(T_1) + g(T_1 \cup S_2)}{2}} \quad \quad (\mathrm{by \ the \ previous \ observation})\\
                                                                                    &= \dfrac{1}{2}\rbk{\avg_{T_1 \subseteq S_1}{g(T_1)} + \avg_{T_1 \subseteq S_1}{g(T_1 \cup S_2)}} \\
                                                                                    &= \dfrac{1}{2} \rbk{\avg_{T_1 \subseteq S_1}{g(T_1)} + \avg_{T_1 \subseteq S_1}{h_{S_2}(T_1)}} \\
                                                                                    &\ge \dfrac{1}{2} \rbk{\dfrac{g(\varnothing) + g(S_1)}{2} + \avg_{T_1 \subseteq S_1}{g_{S_2}(T_1)}} \quad \quad (\mathrm{by \ the \ previous \ lemma}) \\
                                                                                    &\ge \dfrac{1}{2} \rbk{\dfrac{g(\varnothing) + g(S_1)}{2} + \dfrac{g(S_2) + g(S_2 \cup S_1)}{2}} \quad (\mathrm{by \ the \ previous \ obs.}) \\
                                                                                    &= \dfrac{g(\varnothing) + g(S_1) + g(S_2) + g(S_1 \cup S_2)}{4}
            \end{split}
        \end{equation*}
    \end{proof}

    \begin{framedthm}{}
        Given $n \in \N$, let $\func{f}{\powerset([n])}{\R}$ be a submodular, non-negative and symmetric function. Given $\displaystyle S^* \in \argmax_{S \subseteq [n]}{f(S)}$, if $R$ is chosen from $[n]$ uniformly at random (UAR), it holds that $$\Exp \sbk{f(R)} \ge \dfrac{1}{2}f(S^*)$$
    \end{framedthm}
    
    \begin{proof}
        If we apply the previous lemma on $S^*$ and $[n] - S^*$, we get that
        \begin{equation*}
            \begin{split}
                \dfrac{f(\varnothing) + f(S^*) + f([n] - S^*) + f([n])}{4} &\le \avg_{T_1 \subseteq S^*}{\avg_{T_2 \subseteq [n] - S^*}{f(T_1 \cup T_2)}} \\
                                                                           &= 2^{- \abs{S^*}} \cdot 2^{- n + \abs{S^*}} \cdot \sum_{T_1 \subseteq S^*}{\sum_{T_2 \subseteq [n] - S^*}{f(T_1 \cup T_2)}} \\
                                                                           &= 2^{-n} \sum_{T_1 \subseteq S^*}{\sum_{T_2 \subseteq [n] - S^*}{f(T_1 \cup T_2)}} \\
                                                                           &= 2^ {-n} \sum_{T \subseteq S^* \cup ([n] - S^*)}{f(T)} \quad \quad (S^* \cap ([n] - S^*) = \varnothing) \\
                                                                           &= 2^{-n} \sum_{T \subseteq [n]}{f(T)} \\
                                                                           &= \avg_{T \subseteq [n]}{f(T)} \\
                                                                           &= \Exp [f(R)]
            \end{split}
        \end{equation*}
        Lastly, observe that
        \begin{equation*}
            \begin{split}
                \Exp[f(R)] &\ge \dfrac{f(\varnothing) + f(S^*) + f([n] - S^*) + f([n])}{4} \\
                           &\ge \dfrac{f(S^*) + f([n] - S^*)}{4} \quad \quad (\mbox{by non-negativity of} \ f) \\
                           &= \dfrac{2 f(S^*)}{4} \quad \quad (\mathrm{by \ symmetry \ of} \ f) \\
                           &= \dfrac{1}{2}f(S^*)
            \end{split}
        \end{equation*}
    \end{proof}


    We observe this proof can be also modified for functions that are non-negative, submodular but \tit{not} symmetric.

    \begin{framedthm}{}
        Given $n \in \N$, let $\func{f}{\powerset([n])}{\R}$ be a submodular and non-negative function. Given $\displaystyle S^* \in \argmax_{S \subseteq [n]}{f(S)}$, if $R$ is chosen from $[n]$ UAR, it holds that $$\Exp \sbk{f(R)} \ge \dfrac{1}{2}f(S^*)$$
    \end{framedthm}

    \begin{proof}
         Consider the proof of the previous theorem; by simply replacing the application of the symmetry of $f$ in the last step with another application of the non-negativity, we obtain a $\tfrac{1}{4}$-approximation --- leaving the rest of the proof unchanged.
    \end{proof}

    In conclusion, we showed that
    
    \begin{itemize}
        \item NN-MI submodular function have a $\rbk{1 - \tfrac{1}{e}}$-approximation
        \item NN-S submodular functions have a $\tfrac{1}{2}$-approximation
        \item NN sumbodular function have a $\tfrac{1}{4}$-approximation
    \end{itemize}

    Submodularity provides a standard approximation bound to many \NPclass-Hard \tit{maximization} problems. Furthermore, these same results can be obtained through \tit{supermodularity} for many \NPclass-Hard \tit{minimization} problems.

    \chapter{Online algorithms}

    TODO \todo{buco}

    An \tbf{online algorithm} processes its input \tit{piece-by-piece} in a sequential manner, without having access to the entire input from the outset. In contrast, a traditional offline algorithm receives \tit{all the input at once} and must then compute a solution. Online algorithms are valuable because they enable solving problems without requiring full knowledge of the data beforehand. The importance of online algorithms has grown significantly with the rise of the Internet and the explosion of big data, where handling massive datasets has made memory management challenging, even for the most advanced supercomputers.

    A practical example of online algorithms is seen in MBM, which is critical for example in online \tbf{advertising services}. Imagine an advertising platform where each user should be matched to \tit{at most one ad per hour}, and each ad can be assigned to \tit{at most one user per hour}. Based on user interests, the platform generates a list of possible ads for each individual. To maximize revenue, it aims to find the \tbf{largest possible matching} between users and ads. This scenario directly corresponds to MBM, where users form one set of the bipartite graph and ads form the other. However, in this context some challenges may arise, for instance

    \begin{itemize}
        \item the sheer volume of users and advertisements makes storing all the data impractical
        \item the set of available users and ads changes continuously, so earlier matches might no longer be optimal
    \end{itemize}

    Online algorithms address both of these issues effectively. Still, because they must make decisions without knowing future inputs, they sometimes produce results that are \tit{not optimal in hindsight}. For instance, suppose that our online algorithm currently knows tha following association graph --- in which the \tit{red} nodes indicate the \tbf{ads} and the \tit{blue} ones indicate the \tbf{users}

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[-,>=stealth,shorten >=1pt,auto,node distance=1.3cm, thick,main node/.style={scale=0.9,circle,draw,font=\sffamily\normalsize}]

            \node[circle, draw, fill=Carmine] (1) []{$A$};
            \node[circle, draw, fill=BlueLagoon] (2) [below of = 1]{$B$};
            \node[circle, draw, fill=Carmine] (3) [right of = 1]{1};

            % \path[every node/.style={font=\sffamily\small}]

            \draw[-] (1) to (2);
            \draw[-] (3) to (2);

            ;
        \end{tikzpicture}
        % \caption{Given the set of red vertices $S$, the green edges represent $\mathrm{cut}(S)$.}
    \end{figure}

    Our algorithm has two options: either match user 1 to ad $A$ or match it to ad $B$. Without loss of generality, assume that user 1 is matched to ad $A$, as shown below

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[-,>=stealth,shorten >=1pt,auto,node distance=1.3cm, thick,main node/.style={scale=0.9,circle,draw,font=\sffamily\normalsize}]

            \node[circle, draw, fill=Carmine] (1) []{$A$};
            \node[circle, draw, fill=BlueLagoon] (2) [below of = 1]{$B$};
            \node[circle, draw, fill=Carmine] (3) [right of = 1]{1};

            % \path[every node/.style={font=\sffamily\small}]

            \draw[-] (1) [green] to (2);
            \draw[-] (3) to (2);

            ;
        \end{tikzpicture}
        % \caption{Given the set of red vertices $S$, the green edges represent $\mathrm{cut}(S)$.}
    \end{figure}

    Now, suppose that a new user 2 joins our service. After analyzing the preferences of the users, the system determines that only ad $A$ is compatible with user 2, meaning that the latter cannot be assigned any ad since ad $A$ is already matched with user 1.

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[-,>=stealth,shorten >=1pt,auto,node distance=1.3cm, thick,main node/.style={scale=0.9,circle,draw,font=\sffamily\normalsize}]

            \node[circle, draw, fill=Carmine] (1) []{$A$};
            \node[circle, draw, fill=BlueLagoon] (2) [below of = 1]{$B$};
            \node[circle, draw, fill=Carmine] (3) [right of = 1]{1};
            \node[circle, draw, fill=BlueLagoon] (4) [below of = 2]{2};

            % \path[every node/.style={font=\sffamily\small}]

            \draw[-] (1) [green] to (2);
            \draw[-] (3) to (2);
            \draw[-] (1) to (4);

            ;
        \end{tikzpicture}
        % \caption{Given the set of red vertices $S$, the green edges represent $\mathrm{cut}(S)$.}
    \end{figure}

    However, in hindsight we see that this is \tit{not} the optimal solution: in fact, if our algorithm had matched user 1 to ad $B$, we could've been able to match both users.

    This tendency toward non-optimal solutions is a \tit{defining feature} of online algorithms and is often \tit{unavoidable}. As a result, online algorithms are typically evaluated through \tbf{competitive analysis}, which measures the performance of an online algorithm relative to an optimal offline algorithm. The offline algorithm has the advantage of seeing the entire sequence of requests beforehand, while the online algorithm must respond to an \tit{unpredictable sequence}, making decisions without knowledge of future inputs. Unlike traditional worst-case analysis --- which evaluates an algorithm's behavior only on particularly difficult inputs --- competitive analysis demands strong performance across both easy and hard inputs, where difficulty is determined based on how well the optimal offline algorithm performs.

    To formally assess the competitiveness of online algorithms, researchers employ the \tbf{adversary model}, in which an adversary provides \tit{deliberately challenging inputs} to the algorithm. In the case of deterministic algorithms, the adversary is assumed to have \tit{complete knowledge} of the algorithm's internal workings. Given that the online algorithm cannot anticipate future inputs, this all-knowing adversary can exploit its decisions and lead it into making suboptimal choices.

    In particular, we are going to employ such adversary model to prove the following result.

    \begin{framedthm}[label={mbm det}]{}
        There is no deterministic online algorithm for MBM that can match more than $\tfrac{1}{2}$ of the edges of an optimal solution.
    \end{framedthm}

    \begin{proof}
        We prove the result through an adversarial argument. On the first round, the adversary sends three vertices $a$, $b$, $x$, and two edges $\{a, x\}$ and $\{b, x\}$. The online algorithm has then 3 possible choices for $x$.

        \begin{enumerate}
            \item Match $x$ with $a$. On the next round, the adversary sends a new vertex $y$ and an edge $\{a, y\}$. Since $x$ has been matched to $a$, the new vertex $y$ cannot be matched, meaning that the algorithm returns a solution with a matching having size 1. However, an offline optimal solution would have returned the matching $\{\{b, x\}, \{a, y\}\}$, which has size 2. Therefore, the competitive ratio is $\tfrac{1}{2}$.
            \item Match $x$ with $b$. Similarly to the previous case, on the next round the adversary sends a new vertex $y$ and an edge $\{b, y\}$. Since $x$ has been matched to $b$, the new vertex $y$ cannot be matched, but an offline solution would have returned the matching $\{\{a, x\}, \{b, y\}\}$. Therefore, the competitive ratio is still $\tfrac{1}{2}$.
            \item Leave $x$ unmatched. On the next round, the adversary sends a new vertex $y$ and an edge $\{a, y\}$. Now our algorithm has two options.

                \begin{itemize}
                    \item Match $y$ with $a$; this solution is suboptimal, because an offline algorithm would have matched both $x$ and $y$. Hence, in this case the competitive ratio is again $\tfrac{1}{2}$
                    \item Leave $y$ unmatched; this solution is suboptimal, because an offline algorithm would have matched both $x$ and $y$. Hence, in this case the competitive ratio is 0
                \end{itemize}
        \end{enumerate}

        This concludes that the adversary can always send a set of vertices and edges such that the competitive ratio is at most $\tfrac{1}{2}$.
    \end{proof}

    TODO \todo{buco}

    The previous theorem, together with the following proposition, imply that this online greedy algorithm is an optimal approximation for MBM.

    \begin{framedprop}{}
        Given a graph $G$, a maximum matching $M^*$ on $G$, and a maximal matching $M$ of $G$, we have that $$\abs M \ge \dfrac{1}{2} \abs{M^*}$$
    \end{framedprop}

    \begin{proof}
        Let $S := \{v \in V(G) \mid \exists e \in M : v \in e\}$ be the set of vertices of $G$ that are matched by edges of $M$. Hence, by definition we have that $\abs S = 2 \abs M$.

        We observe that:

        \begin{itemize}
            \item each node of $S$ is incident to at most one edge of $M^*$, otherwise $M^*$ would not have been a matching
            \item each edge $e \in M^*$ must be incident to at least one node of $S$, since otherwise $M \cup \{e\}$ would be matching that extends $M$, contradicting the fact that it was maximal
        \end{itemize}

        Now, consider a function $\func{f}{M^*}{S}$ such that $f(e) \in S \cap e$: the first observation implies that $f$ is well-defined, while the second observation implies that $f$ is \tit{injective}. In particular, this concludes that $$\abs{M^*} \le \abs S = 2 \abs M \iff \abs M \ge \dfrac{1}{2} \abs{M^*}$$
    \end{proof}

    \section{TODO}

    In the previous section we discussed how \tit{deterministic} online algorithms must contend with an all-knowing adversary when compared to their offline counterparts. To overcome some of these limitations, \tbf{randomness} can be introduced into the algorithm's behavior. In randomized online algorithms, the nature of the adversary varies depending on how much information they possess. A commonly studied scenario involves a randomized adversary, where the input sequence is generated according to a \tit{random distribution}. However, even in this setting we can still prove the following result.
    
    \begin{framedthm}{}
        There is no randomized online algorithm for MBM that can expectedly match more than $\tfrac{3}{4}$ of the edges of an optimal solution for graphs chosen from a random distribution.
    \end{framedthm}

    \begin{proof}
        Let $G_1$ be a graph such that $E(G_1) = \{\{a, x\}, \{b, x\}, \{b, y\}\}$, and let $G_2$ be the graph such that $E(G_2) = \{\{a, x\}, \{b, x\}, \{a, y\}\}$. Now, consider a distribution that assigns probability $\tfrac{1}{2}$ to both $G_1$ and $G_2$, and probability 0 to all the othe possible graphs.

        On the first round, the adversary sends the edges $\{a, x\}$ and $\{b, x\}$; then, our algorithm will match $x$ to either $a$ or $b$ \todo{what if x unmatched?}, and clearly it will learn nothing about the vertex $y$ whose edges will be sent by the adversary on the next round --- namely, they can send either $\{b, y\}$ or $\{a, y\}$. Through a similar argument used to prove \cref{mbm det}, we see that the probability of $y$ being impossible to match on the next round is $\tfrac{1}{2}$. This implies that $$\Exp[\mbox{number of matched edges}] = TODO$$ TODO \todo{prev eq} However, an offline deterministic algorithm (which is a also randomized algorithm that makes no random choices) can always retun an optimal solution with 2 amtched edges, therefore the expected ratio is $$\dfrac{\tfrac{3}{2}}{2} = \dfrac{3}{4}$$
    \end{proof}

    \printbibliography % UNCOMMENT FOR BIBLIOGRAPHY

\end{document}
